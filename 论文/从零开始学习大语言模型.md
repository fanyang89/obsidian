# 从零开始学习大语言模型

<table><tr><td>修订时间</td><td>修订者</td><td>描述</td></tr><tr><td>2025年7月17日</td><td>Yang Fan</td><td>初稿</td></tr><tr><td></td><td></td><td></td></tr></table>

阅读本文需要基础的微积分和线性代数知识，笔者水平有限，敬请指正。

AI内容声明：本文的写作得到了Qwen3的润色和纠错，大纲、最终校对以及图表、引用等由笔者完成。

大部分的图来自d2l和Build a Large Language Model (From Scratch)

# 基本概念

# 深度学习

深度学习是机器学习的一个分支，其核心在于通过神经网络架构实现数据的表征学习。近年来兴起的大语言模型（LLM）是基于深度学习的大规模神经网络模型（参数规模上常达数十亿至数万亿，Kimi K2已经来到万亿级别），专门用于自然语言的理解与生成。

# 矩阵、张量

矩阵（Matrix）是 $M\cdot N$ 个数，一个二维的数字网格- 张量（Tensor）就是高阶矩阵

![](media/3083e1a5d30584a323c9a64bbd0d2988a2cf65accb04d465eb692f95e36a5978.jpg)

从左到右：标量、行向量、列向量、矩阵、张量

生活中的矩阵和张量：

生活中的矩阵和张量：- 一张黑白的图片，白色像素点为 1，黑色为 0，可以用一个仅包含 0 和 1 的矩阵表示- 一张彩色的图片，每个像素点使用 RGB 三个通道表示，也就是一个像素点对应三个数字，可以用一个三维张量表示- 一个彩色视频可以用一个四维张量表示，额外增加的维度对应时间轴（即帧序列）

# 模型

我们可以将一个模型理解为一个包含大量参数的复杂函数 $y = f(x)$ 。例如，在图像分类模型中，其输入 $x$ 是一张用张量表示的图片，输出 $y$ 则是一个表示图片属于各个类别的概率向量。

最简单的模型是线性的： $y = f(x) = wx + b$ ，其中 $W$ 是权重， $b$ 是偏置。显而易见，仅凭一条“直线”，是无法拟合现实世界中千变万化的复杂关系的。要解决这个问题，激活函数 $\sigma$ 便是一个强有力的工具。它为简单的线性变换注入了非线性特性，从而极大地提升了模型的表达能力。

假设我们套上激活函数sigmoid，令 $\sigma =$ sigmoid， $y = f(x) = \sigma (wx + b) = \frac{1}{1 + e^{- (wx + b)}}$ 一条直线就会变成这样：

![](media/71e1d9f53df1232af601226adf60a2a266dd03e204809b53fbecd819f8674f05.jpg)

点击链接看图， $y = f(x)$ 不再是一条直线

另一个常见的激活函数softmax的函数图像如下：

![](media/ffca22da6261c7ca4a5e98dfd433a3dfbb3fda5d2f16fd03ee4b2e383ebc218e.jpg)

神经网络之所以能够拟合复杂的非线性关系，核心在于激活函数的运用。激活函数的主要作用是在网络中引入非线性。它对每个神经元的线性运算结果进行一次非线性变换。如果没有激活函数，多层神经网络本质上仍是一个线性模型，其表达能力将大打折扣。正是这种非线性变换，极大地增强了模型的表达能力，使其能够逼近任意复杂的函数。其次，为了让模型能够通过梯度下降法进行有效训练，激活函数通常需要具备良好的数学性质，尤其是连续可导（或至少分段可导，如ReLU）。这保证了在反向传播过程中可以顺利计算梯度，从而更新网络参数。

# 通过例子理解模型、训练和推理的关系

让我们尝试使用模型来预测一年内每个月的冰淇淋销量。不妨假设冰淇淋在每年七月卖得最好，其他时间销量低；温度越高，销量越高，温度越低，销量越低。不妨建模为一元二次函数：定义销量 $y = f(x) = ax^2 +bx + c$ ， $x$ 为月份数。

![](media/b455a3b7e4c127f41f245d02b9586113fdf05dd774b53a75c925a9a00e5a395c.jpg)  
上图为 $f(x) = -x(x - 11)$

我们通过损失函数（loss function）来评价一个模型的优劣：损失越小，跟实际越接近，模型越好。不妨定义损失函数为预测值与实际值差的绝对值：Loss = |predict - real|.

但是，数感好的的小伙伴就会发现，这个绝对值要咋算嘛。没错，这个地方用绝对值性质并不好，既不好求导，也不好计算，甚至在计算机里计算也并不高效。那么如何替换掉绝对值呢？可以简单的使用平方： $\frac{1}{2} x^{2}$ ， $f^{\prime}(x) = x$ 。

我们通常使用的损失函数 $L$ 实际上是这样的：

$$
L(\mathbf{w},b) = \frac{1}{n}\sum_{i = 1}^{n}i^{(i)}(\mathbf{w},b) = \frac{1}{n}\sum_{i = 1}^{n}\frac{1}{2}\left(\mathbf{w}^{\top}\mathbf{x}^{(i)} + b - y^{(i)}\right)^{2}.
$$

其中， $\mathbf{w}^{\top}\mathbf{x}^{(i)} + b$ 是预测值， $y^{(i)}$ 是实际值

训练。为了进行实际的预测，我们还需要确定参数 $(a,b,c)$ 的值。此时我们搬来历年的冰淇淋销量 $y$ 与月份 $x$ 的关系数据，挨个输入到模型里，不断调整 $a,b,c$ 三者的值，使得损失 $L(a,b,c)$ 最小。推理。现在 $a,b,c$ 已经确定，网络 $y = f(x)$ 也建好了，我们就可以开始进行实际的冰淇淋销量预测了：根据月份 $x$ 计算出 $y$ 的值），计算略。

# 梯度下降、学习率、随机梯度下降

梯度：多元函数在某一点处由所有偏导数组成的向量，向量方向为函数值增长最快的方向，其模为该方向的变化率。

梯度下降：计算损失函数梯度确定最陡下降方向，迭代更新参数以逼近损失函数最小值（不是极小值）。标准的梯度下降在每次参数更新时需要计算整个训练集（所有 $N$ 个样本）上的平均梯度。这在数据集巨大时计算成本极高，每次迭代耗时过长。

$$
\mathbf{a}_{n + 1} = \mathbf{a}_n - \eta \nabla f(\mathbf{a}_n)
$$

其中

- $a_{n + 1}$ 表示第 $n + 1$ 次迭代时的参数值- $a_n$ 表示第 $n$ 次迭代时的参数值- $\eta$ 表示学习率。下面会详细展开。- $\nabla f(\mathbf{a}_n)$ 表示 $f(x)$ ( $f(x)$ 在这里表示损失函数 $L(\mathbf{w},b)$ ) 在 $x = a_n$ 上的梯度，也就是损失增长最快的方向，取负号就是下降最快的方向。我们训练的目标就是为了降低损失：训练时，取 loss 下降最快的方向更新参数；而进行对抗攻击时（比如给样本增加扰动信号防止被训练，MIST），构造样本使得 loss 上升更快，以误导网络。

随机梯度下降（SGD）：SGD 是一个每次迭代仅使用少量样本来近似梯度并更新模型参数的优化算法，在高维或大数据场景下显著降低了计算成本，尤其适用于大规模数据集的训练。实践中，严格使用单个样本的纯 SGD 较少见，更常见的是 Mini- batch SGD，每次迭代使用一个小批量（batch size 通常取 32、64、128、256 等）的样本计算平均梯度。这在一定程度上平衡了计算效率和梯度估计的方差。Batch size 是一个重要的超参数。

$$
(\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \frac{\eta}{|B|} \sum_{i \in B} \partial_{(\mathbf{w}, b)} l^{(i)} (\mathbf{w}, b).
$$

学习率：学习率是机器学习优化算法中至关重要的超参数（Hyperparameter），控制着每次迭代中模型参数根据计算梯度进行更新的步长大小，其取值直接影响训练过程的收敛速度、稳定性和最终模型性能：过高的学习率可能导致训练震荡甚至发散，乃至于无法收敛；过低的学习率则会使收敛过程极其缓慢或停滞在次优点。

题外话。对于我们存储工程师来说，可以忘记书上那些复杂的推导，直接用

microsoft/nni、optuna 自动调整超参数（学习率、batch size 等），加上自适应优化器 Adam

它对初始学习率容忍度更大，可显著降低调参难度——即可开始炼丹。你只需要一些数

据集，一张卡和一些简单的代码，就可以开始构建自己的模型了。

学习率过大，可能导致参数更新步长过大，越过最优值（ $x = 0$ ）导致震荡甚至发散，无法收敛：

![](media/51c22bb9b701313b6c982d91f076681ce61534f24dbb02a96ee9b6a0b8459b82.jpg)

学习率过小，则会导致收敛速度变慢，需要更多迭代次数才能接近最优解。如下所示，尽管经过了 10 个步骤，我们仍然离最优解很远：

![](media/30f1802666a71ce313b0d229b3fa43ed352df1a8caad41fa051d963742bb1773.jpg)

# 自动微分

题外话：以工程角度，此节可以跳过，所有的深度学习框架都帮你做了这部分。比如在PyTorch里，只需要简单的调用.backward()即可。放在这里是因为这个部分在历史上非常的重要，这是Geoffrey Hinton（图灵奖、诺贝尔物理学奖得主、被誉为深度学习教父）的重要贡献之一。

训练模型的目标是调整其参数以最小化损失。这一过程通常依赖于基于梯度的优化算法（如随机梯度下降，SGD）来逐步更新参数。因此，高效、准确地计算梯度便成为关键所在。

对于深度学习中复杂的模型，手动推导梯度公式几乎不现实。自动微分（Automatic

对于深度学习中复杂的模型，手动推导梯度公式几乎不现实。自动微分（Automatic Differentiation, AD）技术正是为了解决这个问题而设计的。自动求导的关键在于：将复杂的函数分解为一系列基础算子（Operator），并为每个算子预先定义好其导数的计算规则。在计算时，自动求导基于链式法则，通过组合这些预定义的局部导数，来精确计算整个函数的梯度。这些算子的具体实现可以利用各种高效的计算后端进行加速，例如NVIDIA CUDA、AMD HIP、华为CANN、摩尔线程MUSA和寒武纪BANG（最近大厂进了很多货）等硬件加速技术。当然，CPU也可用于计算，只是算力（如通过AVX/AMX指令集）远小于GPU，即便是最新的集成了NPU的CPU（如AMD Ryzen AI，Intel Core Ultra系列），其算力仍然远远低于GPU。

在自动微分的众多实现方式中：

- 反向传播算法（Backpropagation）是深度学习中最核心的梯度计算方法。它本质上是反向模式自动微分在神经网络上的具体应用，能够一次前向传播和一次反向传播就计算出所有参数的梯度，计算效率与模型参数数量无关。- 符号微分（Symbolic Differentiation）会先生成梯度的完整数学表达式，再代入数值进行计算。虽然它能得到精确的解析解，但容易产生表达式膨胀问题，即生成的梯度公式极其庞大

复杂（一会儿我们可以看到实际运用的神经网络的函数有多大），导致编译和计算成本过高。因此，纯粹的符号微分不适合直接用于大规模模型训练，但其思想被现代深度学习框架（如通过计算图）所借鉴和优化。

- 数值微分（Numerical Differentiation），如有限差分法，通过微小的参数扰动来近似计算梯度。它虽然实现简单，但计算成本极高（每个参数都需要额外的前向传播），且存在舍入误差导致精度有限。因此，它在实践中仅用于验证反向传播等方法实现的正确性。

在深度学习的训练场景中，梯度计算函数具有一个显著特点：它的输入是模型中成千上万甚至数以亿计的可训练参数，而输出通常是一个单一的标量损失值。正是这种“多输入对单输出”的函数结构，使得反向传播显得尤为高效：它仅需一次前向传播和一次反向传播，便能一次性计算出最终损失对所有模型参数的梯度，其计算成本与参数数量基本无关。作为对比，正向模式自动微分需要对每个参数都进行一次独立的计算才能得到其梯度，在参数量巨大的深度学习模型中，这种方法的计算成本是无法接受的。

下面我们举例说明反向传播算法。对于复合函数 $y = f(x),a = A(x),b = B(a),y = C(b)$ ，也就是 $y = C(B(A(x)))$ ，根据链式法则（复合函数的导数可以分解为各组成函数导数的乘积），我们可以知道 $y$ 对 $x$ 的导数（导数值 $\frac{dy}{dx}$ 永远为1）

$$
\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{\mathrm{d}y}{\mathrm{d}y}\frac{\mathrm{d}y}{\mathrm{d}b}\frac{\mathrm{d}b}{\mathrm{d}a}\frac{\mathrm{d}a}{\mathrm{d}x}
$$

# 算一算

不妨假设

$$
\begin{array}{rcl}{y=f(x)}&{=}&{C(B(A(x)))}\\{A(x)}&{=}&{x^2}\\{B(a)}&{=}&{a^3+5}\\{C(b)}&{=}&{2b}\end{array}
$$

现在，我们要计算 $x = 2$ 时， $y$ 对 $x$ 的导数， $\left.\frac{\mathrm{d}y}{\mathrm{d}x}\right|_{x = 2}$ 首先代入 $x = 2$ ，进行正向的函数值 $y = f(x)$ 计算：

$$
\begin{array}{rcl}{a}&{=}&{A(x)=2^2=4}\\{b}&{=}&{B(a)=4^3+5=69}\\{c}&{=}&{C(b)=2\times 69=138}\end{array}
$$

然后，我们从最终输出 y 开始，反向计算梯度：

$$
\begin{array}{rcl}\frac{\mathrm{d}y}{\mathrm{d}b}\bigg|_{b = 69} & = & 2 \\ \frac{\mathrm{d}b}{\mathrm{d}a}\bigg|_{a = 4} & = & 3a^2 = 3\times 4^2 = 48 \\ \frac{\mathrm{d}a}{\mathrm{d}x}\bigg|_{x = 2} & = & 2x = 4 \end{array}
$$

注意，这里的 a 使用到了上一次正向计算时计算得到的 a 的值。

根据链式法则，把他们乘在一起：

$$
\left.\frac{\mathrm{d}y}{\mathrm{d}x}\right|_{x = 2} = 2\times 48\times 4 = 384
$$

使用 PyTorch 进行验算：

$- 3\frac{9}{10}$ python Python 3.13.5（main，Jun 21 2025，09:35:00）[GCC 15.1.1 20250425] on linux Type "help"，"copyright"，"credits" or "license" for more information. >>> import torch >>> x $\equiv$ torch.tensor([2.0]，requires_grad=True) >>> a = x\*\*2 >>> b = a\*\*3 + 5 >>> c = 2\*b >>> c.backward() >>> x.grad tensor([384. ]）

通过上述示例，我们已经掌握反向传播的核心思想，以及反向传播相比于正向模式自动微分的区别，以及与其他自动微分方式的区别。自动微分是深度学习框架的核心原理。

# 参数和量化

![](media/4ad9c4479887bed3ab54ef05b65484333ebad3e48f17afd56356fe8784907167.jpg)  
float量化到int4

在标准的模型训练过程中，模型的权重和激活值通常使用float32（单精度浮点数，f32）进行存储和计算。为了优化，有时也会采用float16或bfloat16（值域为f32，但精度比f16低）进行混合精度训练。量化（Quantization）是一种关键的模型推理优化技术。它的本质并非简单的类型转换，而是通过一个线性或非线性的映射函数，将高精度的浮点数张量近似地表示为低比特的整型张量。

以最常见的线性int8量化为例，该过程通常涉及两个关键参数：缩放因子（scale）和零点（zero- point）。它们共同定义了如何将一段浮点数范围（例如从- 10.5到8.3）映射到int8所能表示的整数范围（- 128到127）。通过这个映射，每个浮点数都能找到一个对应的近似整数值。

# 量化的优点：

·减小模型体积与显存占用。模型参数通常以float32（4字节）格式存储。int8量化后，每个参数仅需1个字节，模型体积能直接缩小为原来的1/4。对于动辑数千万参数（如ResNet- 50,25.6M，Million）甚至数千亿参数（如DeepSeek- V2,236B，Billion）的现代模型。这意味着：

模型文件更小，便于分发和部署加载模型进行推理时，占用的VRAM大幅降低，使大模型边缘场景下运行成为可能加快推理速度，提升吞吐量：在现代处理器上，执行整型运算通常比执行浮点运算速度更快、能效更高；由于参数和激活值的位宽降低，单位时间内可以从内存加载更多的数据到计算单元

计算效率。整型的计算比浮点数的计算快很多·提高吞吐。量化为整型后，由于每个数字位宽的降低，吞吐量得到了提升；通常整型的吞吐上限本身也高于浮点。

# 量化的缺点：

量化的缺点：- 潜在的精度损失。量化的核心是将连续的浮点数范围映射到有限的离散整数点上。这个近似过程会引入量化误差，可能导致模型推理精度的下降。就像用一把只有厘米刻度的尺子去测量毫米级的物体，必然会丢失一部分信息。不过，这种精度损失并非不可接受，通过量化感

知识练（Quantization- Aware Training）等技术，可以在训练阶段让模型适应这种精度损失，从而在保持极高性能的同时，将精度下降控制在极小范围（~1%）

# 多层感知机

多层感知机（MLP）是深度学习基础且重要的入门模型之一，其发展离不开Hinton等先驱的卓越贡献。它由多层神经元组成，除了输入层，每一层的神经元都与上一层的所有神经元相连，接收其输出作为输入，并将自身的输出传递给下一层。在每个神经元内部，会将所有输入进行加权求和，加上一个偏置，再通过一个非线性激活函数（如Sigmoid或ReLU）进行处理，得到最终的输出值。网络中，输入层负责接收原始数据，中间的隐藏层进行逐级特征提取，最终的输出层给出预测结果。现在计算机系学生常做的手写数字识别（MNIST）项目，就常用多层感知机来作为基础模型（现在一般用CNN实现，性能更好）。

如下是一个单隐藏层（不是输入输出层的层都是隐藏层）的多层感知机：

![](media/20379a2b40b965fba4ad2489a9e696ae9fc065aab70da991bb17026ce75428f8.jpg)  
图4.1.1：一个单隐藏层的多层感知机，具有5个隐藏单元

这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。因此，这个多层感知机中的层数为2。注意，这两个层都是全连接的。每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。

到这里也可以理解了，手动对这样一个复杂的复合函数求导不是不可能，只是需要一些汗水，计算机中的“计算”二字在此体现得淋漓尽致。实际运用中的神经网络不会像上图这么简单，如下是一个复杂一点的神经网络的一部分（图来自Recursive Neural Networks with PyTorch）：

![](media/c1f6a1f7bf4dbcb470fcede917bac5ff50d3c136ac1214f388d5656fe37fe0c2.jpg)

- 本文略去CNN的介绍，有兴趣的读者可以自行了解。

# RNN 循环神经网络

- 本节可以跳过：Transformer比RNN具有显著优势（秒子）

RNN（Recurrent Neural Network），即循环神经网络，是为处理序列数据而设计的神经网络。它的核心在于通过引入“循环”结构来赋予网络“记忆”的能力，从而克服了传统前馈神经网络（Feed- Forward Neural Networks）中含输入间相互独立的局限性。具体来说，RNN在处理序列中的每一个元素时，不仅会考虑当前的输入，还会接收并融合上一个时间步传递过来的隐藏状态（Hidden State）。这个隐藏状态就像是网络对过去信息的浓缩摘要，它与当前输入共同参与计算，一起决定了当前时刻的输出，并生成新的隐藏状态传递给下一个时间步。正是这种能够捕捉时序动态的机制，使得RNN在自然语言处理（如机器翻译、文本生成）、语音识别和时间序列预测等领域发挥着至关重要的作用。

![](media/6b5af06d8f47919b505871500e72936f38a401109e8b087f6f43dfc53a28ecfe.jpg)  
图8.4.1：具有隐状态的循环神经网络

局限性：长程依赖问题（The Long- Term Dependency Problem）是RNN最致命的弱点。由于信息一样一步步向后传递的，当序列很长时（比如一篇长文章），模型很难将开头的信息有效传递到结尾。信息在传递过程中会不断被稀释和遗忘，导致梯度消失（模型学不到长期规律）或梯度爆炸（训练不稳定）。虽然LSTM和GRU等变体在一定程度上缓解了这个问题，但并未从根本上解决。

而Transformer抛弃了循环结构，采用自注意力机制，允许模型在处理一个词时，能够直接关注到输入序列中的任何其他词，并计算它们之间的关联强度。这意味着，无论两个词在序列中相隔多远，它们之间的信息通路都是直接的，从而完美地解决了长距离依赖问题。后面有章节专门讲解自注意力机制。

# 卡间并行策略

![](media/0336c792fc6b960f4d5f81cb065b13239e62c6c1acc8b5bce8f7bc8f708e33fd.jpg)  
Fig. 3: Existing parallelism for distributed training

从左到右：数据并行，张量并行，层间并行

数据并行（DataParallelism，DP）将同一个模型完整地复制到每个GPU上，将小的数据批次（mini- batch）切分成更小的微批次（micro- batch），并将每个微批次分配给不同的计算设备上并行处理。

在每次迭代中，模型的副本在不同设备上使用不同的数据子集进行前向传播和反向传播计算，所有设备都会独立计算它们所负责的数据批次的梯度，然后通过NCCL提供的AllReduce聚合梯度，更新模型参数。

优点：

- 实现简单：逻辑清晰，易于集成到现有训练框架中（如PyTorch的DistributedDataParallel）- 普适性强：对模型结构没有特殊要求，几乎所有模型都适用

缺点：

- 内存冗余：最大的缺点。每个GPU都必须能容纳下完整的模型、梯度和优化器状态。当模型大到单个GPU无法装下时，DP就无能为力了。- 通信瓶颈：通信开销与模型参数量成正比。每次迭代都需要传输整个模型的梯度，当模型巨大或节点间带宽有限时，这会严重拖慢训练速度。

- 张量并行（TensorParallelism，TP）：将权重矩阵和中间激活值在不同维度上进行分割，并在多个GPU上并行计算，最后通过通信操作（如AllReduce、AllGather等）将结果合并

优点：

- 节省内存：将大张量分散到多个设备，使得单个设备不再需要存储完整的模型层，有效解决了单卡显存不足的问题- 通信高效：相比DP，TP的通信内容通常是中间的激活值，而非整个模型的梯度，在某些情况下通信量可能更小

缺点：

- 实现复杂：需要对模型算子的底层实现进行深度修改，不能直接应用在任意模型上。通常需要专门的框架（如NVIDIA/Megatron-LM）来支持- 通信频繁：通信发生在模型的每一层（或特定层）的内部，通信频率远高于DP，对设备间的通信带宽和延迟要求极高（通常需要NVLink）

- 层间并行（PipelineParallelism，PP）：神经网络模型的不同层划分为若干个阶段，每个阶段可以在不同的GPU计算设备上执行。

$=$ 优点：

- 节省内存：和TP一样，它解决了单卡显存不足的问题，每个设备只需存储模型的一部分。- 通信量可控：通信只发生在各个阶段的边界处，通信内容是层的激活值。相比TP，通信频率低得多。

缺点：

- 流水线气泡(Pipeline Bubble)：在流水线启动和排空阶段，部分GPU会处于空闲等待状态，导致硬件利用率下降。微批次越多，气泡占比越小，但也会增加中间激活值的缓存压力

- 负载均衡困难：如何将模型切分成计算量几乎相等的阶段是一个难题。如果某个阶段的计算时间远长于其他阶段，它将成为整个流水线的瓶颈。- 激活值内存：中间阶段的设备需要缓存所有inflight微批次的激活值，以便在反向传播时使用，这可能导致显存占用过高。

# 大语言模型

作为当前全球科技界关注的焦点领域，大语言模型（Large Language Model，LLM）不仅掀起了人工智能技术的革命性突破浪潮，更被视为通向通用人工智能（AGI）的重要技术路径。其技术演进的关键里程碑包括Transformer架构的提出以及GPT系列模型的迭代演进。本节将重点解析这些技术演进的关键节点。

# Transformer

2017年6月12日17点57分，Google一篇名为Attention Is All You Need的论文横空出世，目前引用次数已经来到了\~18万次，是目前所有大语言模型的基石。这篇论文发表在NeurIPS2017上，然而没有获得Oral的资格（Reviewer觉得好的论文可以进行presentation，大会上口头报告）。

![](media/8ae0c839cd844c1827f8ed429637c056b2a07a8cfcd8c8c9731d318a2d6d50bb.jpg)

上图为Transformer变形金刚，Transformer论文作者很喜欢玩梗

Transformer架构是一种完全依赖自注意力机制（Self- attention，后面会深入讲解）的深度学习模型，通过编码器（Encoder）和解码器（Decoder）的堆叠结构实现序列到序列的建模，彻底摒弃了传统RNN和卷积的依赖，能够高效捕捉长距离语义关联。我们常用的GPT（生成式预训练Transformer）等，都是Transformer的一个变体。

Transformer 是一种开创性的深度学习架构，其核心完全基于自注意力机制（Self- Attention）。它通常采用编码器（Encoder）与解码器（Decoder）的堆叠结构来进行序列到序列（Seq2Seq）的建模。这一设计的突破之处在于，它摆脱了传统模型（如 RNN）对顺序计算的依赖，使其能够并行处理整个序列，能极其高效地捕捉文本中的长距离语义关联。如今广为人知的各类大语言模型，例如作为解码器应用的 GPT 系列和作为编码器应用的 BERT，都是其强大的变体。下面是 Transformer 的原始架构图（图来自《Build a Large Language Model (From Scratch)》）：

下面是Transformer的原始架构图（图来自《BuildaLargeLanguageModel（FromScratch)》）：

![](media/b859017d7f23824378c551e7de19a6b6f88bcd8f8397c2f339b1c4b2f80d59ec.jpg)

图 1- 4 原始 Transformer 架构的简化描述，这是一种用于机器翻译的深度学习模型。Transformer 由两部分组成：一个是编码器，用于处理输入文本并生成文本嵌入（一种能够在不同维度中捕获许多不同因素的数值表示）；另一个是解码器，用于使用这些文本嵌入逐词生成翻译后的文本。请注意，图中展示的是翻译过程的最后阶段，此时解码器根据原始输入文本（“This is an example”）和部分翻译的句子（“Das ist ein”），生成最后一个单词（“Beispiel”）以完成翻译

Transformer的目标是将输入的Token序列转换成输出序列。

从图中，我们可以一瞥 Transformer 的工作流程（论文中的描述更复杂）：

# 预处理

输入文本（也就是平常大家在 ChatGPT 聊天框里问的问题）经过预处理后，转换成模型可以处理的格式。

预处理通常需要经过这些步骤：

- 分词：输入序列转换成 Token[]，上图中 "This is an example" 转换成 ["This", "is", "an", "example"]LLM 通常用到的分词算法非常的简单，直接根据空格分词（比如英语），或是 BPE 算法（统计相邻字符对的出现概率并生成合并规则，根据规则列表，逐规则合并配对，生成最终的 token 序列）。

如果你之前接触过 NLP，会发现：

- 传统分词需要词典、语法树、消歧义算法（如最大匹配法、HMM），而 LLM 的 tokenizer 完全通过数据驱动的统计方法（如 BPE）自动生成词汇表，甚至标点符号和特殊字符都会被编码为独立 token- 当输入存在拼写错误（如 "example"）或未登录词时，传统分词会直接报错，而 LLM 的 tokenizer 会将其映射到最接近的子词嵌入（如拆分为 "exam" + "##pel"），这种设计让 LLM 天然具备处理噪声数据的能力

Qwen3 对我上面这段话的评价：这是深度学习对特征工程的降维打击，当模型参数量达到千亿级时，简单的分词反而能让注意力机制更高效地捕获长距离依赖关系，而传统 NLP 中那些精雕细琢的分词规则，在 175B 参数面前显得苍白无力。笔者深有体会：参数的设置，策略的变更，可以在真实的环境中收集运行数据，并基于数据进行决策，效果会非常显著，精雕细琢的启发式规则在海量数据形成的策略面前不堪一击。这是我想进行 ZBS 观测性改进的核心动机，最终通过海量数据的收集、统计和分析，计算出最合理的参数和策略；找到并消除性能瓶颈，提高 QoS。

题外话。笔者在美学评估领域也略有见地，下面分享这个领域的例子：

笔者作为一个摄影爱好者，拥有约 40T 的照片数据，需要从中进行筛选构图优秀，主题明确的作品进行后期处理。这个任务其实并不容易，因为大师的作品往往是这样的，与通常的古典审美大相径庭：

![](media/7684db6add166285d65c4eaa323821ddf585cf05fe9a5a21e5c60215f621d79b.jpg)  
从左到右：分别由史蒂芬·肖尔，森山大道，威廉·埃格尔斯顿拍摄

对于这样量级的数据集，我已难以为续，因此想到了使用美学评估模型处理照片。但传统的美学评估模型，如北邮的EAT模型，采用CNN架构：一张4000px的图片，在预处理时就已经变成了224x224分辨率，大部分的信息已经丢失。当笔者部署这一模型进行筛选时，效果并不理想，甚至选出了很多"烂片"。但理论上，通过多模态模型，如OpenAILCLIP加以大师作品进行微调（在CLIP问世前并不可行，因为few- shot的性质，会导致模型过拟合），就可以实现目标。目前，笔者还在努力实现这一想法。

- 对每个Token进行转换，转换为Token embedding

- 所谓Embedding，就是一个包含了丰富语义信息的稠密向量（Dense Vector）- 大语言模型在预训练后会生成一个庞大的词表。模型会先将每个Token映射到词表中的唯一ID，然后根据ID在预训练好的Embedding矩阵中查找到对应的向量，该向量即为Token Embedding。- 注入位置信息。Token Embedding本身无法表达其在序列中的位置关系，因此需要为其注入位置信息。例如，OpenAI GPT系列模型就采用了绝对位置编码的方案，将Token Embedding与其相应的位置编码向量进行相加，从而让模型能够理解token的顺序。

# 形象的理解 Token embedding

在上一代的 NLP 经典模型 word2vec 中，可以对 word embedding 进行计算，揭示词语间的语义关系。例如：

- “飞机”的 embedding 减去“天空”，加上“海洋”，结果在向量空间中会最接近于“轮船”- “咖啡”的 embedding 减去“苦”，加上“甜”，结果在向量空间中会最接近于“奶茶”

![](media/286df2bb7318855e381d44bca1ce594a75f232b60516d8de07abe190ad2f712b.jpg)

上图为 BAAI/bge- m3 产生的结果，但并不是直接加减然后判断相似度，而是用点积进行距离计算取 top。

笔者多数需要 embedding 的程序由 bge- m3 驱动。

尽管在 LLM 中，我们无法再直接进行这类显式的向量计算，但其内部生成的 Embedding 同样蕴含了这种丰富的、结构化的语义信息，能够捕捉到词语之间复杂的关联。

我们无法在 LLM Tokenizer 生成的 embedding 中直接进行这类直接的（加加减减得到一个新的词）向量计算的原因是：

- Transformer 的多头注意力和前馈网络捕捉的是高度非线性的复杂关系，而不是简单的线性关系- BGE 这类嵌入模型通过对比学习（Contrastive Learning）进行训练，目标是“让相似的句子/词在向量上更近，不相似的更远”，而不是为了保持线性类比结构

# 编码器（Encoder）

编码器的核心作用是将输入的词元序列（Token Sequence），转化为一系列富含上下文信息的向量表示。

其主要流程包括：

- 词嵌入 (Word Embedding): 将输入的离散词元映射为连续的向量。- 位置编码 (Positional Encoding): 向词嵌入中注入每个词元的位置信息，以弥补自注意力机制对序列顺序不敏感的缺陷。- 多层编码器块 (Encoder Blocks): 通过堆叠多个编码器块（每个块包含自注意力机制和前馈神经网络），让模型能够深入捕捉输入文本的全局依赖关系和语义特征。

其中，自注意力 (Self- Attention) 机制是 Transformer 模型捕捉长距离依赖的精髓所在。

自注意力机制与标准注意力的区别：

- 标准注意力 (Attention)，在 Transformer 架构中常被称为交叉注意力 (Cross-Attention)，它关联两个不同的序列。例如，在翻译任务的解码阶段，它帮助目标语言序列（如德语）的每个词元去关注源语言序列（如英语）中所有相关的词元。- 自注意力 (Self-Attention) 则在同一个序列内部进行。它计算序列中每个词元与其他所有词元之间的关联度，从而更好地理解该词元在当前上下文中的确切含义。例如，在处理句子 "The animal didn't cross the street because it was too tired" 时，自注意力机制可以帮助模型判断 "it" 指代的是 "animal" 而不是 "street"。

# 注意力机制

- 关于 KV Cache 和单头注意力机制的更具体一点的例子，详见 KV Cache 一节，本节以讲公式为主。

- 推荐观看 3Blue1Brown 经典视频以帮助理解，其中进行了注意力机制的可视化：【官方双语】直观解释注意力机制，Transformer 的核心

注意力机制是 Transfomer 架构的精华所在。Transformer 使用的是多头注意力（Multi- Head Attention），这一设计给模型在不同位置、从不同表征子空间中关注信息的能力。多头注意力由多个单头注意力机制并行组成。

单头注意力（Single- Head Attention）机制的公式如下：

$$
\mathrm{Attention}(Q,K,V) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Attention 函数，接受三个参数 $Q$ （Query）， $K$ （Key）， $V$ （Value），输出的结果是一个加权后的 $V$ 矩阵，权重由 softmax(...) 计算得出。我们常说的 KV Cache 的 $K,V$ ，其实指的是注意力机制里的 $K,V$ 矩阵。

我们继续看 softmax(...) 括号中的内容。其中 $K^T$ 是 $K$ 矩阵的转置。 $d_k$ 是 Key 向量的维度。用 $QK^T$ 的结果除以 $\sqrt{\square}$ 是为了进行缩放（Scaling），防止 $d_k$ 过大时，点积结果过大，导致 softmax 函数进入梯度极小的饱和区，从而稳定训练过程。

对于输入序列（矩阵）中的每一个 token $x_i$ ，我们都会通过三个不同的权重矩阵 $(W^Q,W^K,W^V)$ 来生成三个新的向量：

- $Q$ ，Query，查询向量。 $q_i$ 代表当前词为了更好地理解自己，主动发出的“查询”或“提问”，它想知道：“句子里的其他词谁和我最相关？”- $K$ ，Key，键向量。 $k_i$ 代表当前词作为一个“信息提供者”，身上贴的“标签”或“关键词”。它用来响应其他词的查询。- $V$ ，Value，值向量。 $v_i$ 代表当前词实际包含的“内容”或“信息”。一旦注意力权重确定，这个向量就会被用来计算最终的输出。

这里的 $W^{Q}, W^{K}, W^{V}$ ，是三个独立的、需要通过模型训练学习到的参数矩阵；对于一个注意力头，这组参数矩阵是唯一的。

多头注意力的核心思想，正是拥有多组这样的 $W^{Q}, W^{K}, W^{V}$ 矩阵，每一组构成一个独立的“头”。最后，将所有“头”的输出结果 $ji$ 进行拼接，并通过一个额外的线性层 $W^{Q}$ 进行融合，形成最终的输出。

对于整个输入矩阵 $X$ （ $n$ 个 token $x_{i}$ 可以组合成一个 $X$ ），我们可以一次性计算（必须强调，计算的时候不是逐个对 token embedding vector 进行 $Q, K, V$ 计算，只需要直接做矩阵乘）出所有的 $Q, K, V$ 矩阵：

$$
\begin{array}{l}\bullet Q = X\cdot W^Q \\ \bullet K = X\cdot W^K \\ \bullet V = X\cdot W^V \end{array}
$$

# 解码器（Decoder）

解码器的核心任务是将编码器（Encoder）提供的源语言信息，生成为目标语言的序列。它的工作模式是自回归的，这意味着它不会一次性输出整个句子，而是像人类写作一样，逐个词元地进行预测和生成。

工作流程如下：

- 接收输入。在生成每一个词元时，解码器会同时整合两个来源的信息：
- 编码器的输出：这是对整个源语言句子（例如中文）的全局上下文表示。
- 已生成的部分：它会参考自己已经生成的目标语言序列（例如德语的前几个词），来预测下一个最可能的词元。- 循环生成：这个“预测-生成-参考”的过程会不断重复。例如，在预测第 5 个德语词时，解码器会结合编码器的全部信息以及它已生成的前 4 个德语词。- 终止条件：当解码器生成 <eos> 标识（end-of-sequence），或达到预设的最大长度时，生成过程便会停止。

最终，解码器输出的是一个完整的目标词元序列（Token Sequence）。

# 输出层

解码器输出的向量会经过最后一步处理，生成可读的文本。

这个过程分为两步：

1. 一个线性变换层（Linear Layer）将解码器输出的高维向量映射到整个词汇表的维度上。这样，在句子的每个位置，我们都会得到一个分数向量（也称为 Logits），其每个元素对应词汇表中一个词语的“可能性”得分
2. 这个分数向量会通过一个 Softmax 层，将分数转化为一个概率分布。此时，概率值最高的词语，就是模型在该位置上最有可能的预测结果。选择过程还会结合 temperature 参数：当

temperature=0 时，选择概率最高的结果，否则会根据 temperature 参数值进行发散，temperature 越高，发散程度越高，越有机会选到概率更低的结果。将每个位置预测出的词语拼接起来，就形成了一个完整的句子（如上图，一个完整的德语句子）。

# GPT 和 BERT 的架构

# GPT

GPT 的生成过程是逐个 token（词元）进行的，这个机制被称为自回归。

具体流程如下：

- 初始预测：模型首先根据你提供的初始输入（Prompt）来预测并生成第一个 token- 更新上下文：接着，模型会将这个新生成的 token 追加到输入文本的末尾，形成一个更长的、更新后的上下文- 循环迭代：模型会基于这个更新后的完整上下文，再次进行预测，生成下一个 token这个“生成- 追加- 再预测”的循环会一直进行下去，直到达到预设的停止条件。当 temperature 参数为 0 时，每次迭代，模型会严格选择概率最高的那个 token。

![](media/5995a20c5ad043fa3e5dedab52741534bba5eaae7abb5ed45f52700e195ea0b1.jpg)  
图1-8 GPT架构仅使用原始的Transformer解码器部分。它被设计为单向的从左到右处理，这使得它非常适合文本生成和下一单词预测任务，可以逐个词地迭代生成文本

# ChatGPT

ChatGPT是OpenAI基于GPT系列模型构建的产品。OpenAI ChatGPT最大的贡献在于：通过技术整合与工程化创新，首次把海量的数据（Scaling Law）塞进了模型，使得智能开始涌现，人工智能开始落地。

对于Scaling Law，最直观的理解是：计算量C和模型性能满足幂律关系，模型越大，模型性能（这里的性能不是计算性能，是模型本身的表现）越强。详情请参考：GPT- 4 Technical Report

![](media/0a1cc0ac2d3044f285049e05d8a996dd7678963fcd09f3d6a72b7942bb6423bb.jpg)  
OpenAl codebase next word prediction

题外话。其实对于Scaling law，百度亦有贡献：当时有一批人都意识到了Scaling Law的存在，如llya Sutskever（原OpenAI首席科学家、创始人之一，Hinton大弟子）、Rich Sutton（强化学习之父）等。

相关新闻整理如下：

- 2016年3月，Google DeepMind AlphaGo战胜围棋九段李世石- 2016年11月，OpenAI开始进行大规模强化学习研究，开发机器人所采用的算法OpenAI Five- 2017年1月，百度任命陆奇为百度集团总裁兼首席运营官，推行“All in AI”的人工智能先行战略- 2017年12月，百度的论文Deep Learning Scaling is Predictable, Empirically发布在Arxiv上
- 一作Joel Hestness，2019年离开百度，加入Cerebras
- NVIDIA宣布其在LLaMA 4上达到1000token/s的处理速度，而Cerebras公布的基准测试结果高达2500token/s
- 二作Sharan Narang，后来去了Meta做LLaMA，是作者之一- 2018年1月22日，李彦宏：从没说过All in AI，多数资源还是在搜索上- 2018年5月18日，百度宣布陆奇因身体和家庭原因，从7月起卸任总裁和COO
- 百度发布陆奇职位变动的消息后，百度股价周五收跌 $9.54\%$ ，市值蒸发约75亿美元- 2019年2月，OpenAI发布GPT- 2- 2019年4月，OpenAI Five最后一次公开展示
- 三局两胜制的Dota2比赛中击败了2018年国际邀请赛冠军OG

2019年7月，微软向当时仍是非营利组织的OpenAI投资10亿美元

2022年11月，OpenAI发布ChatGPT

2023年3月，百度发布文心一言

# BERT

BERT与GPT类似，区别在于BERT是做完形填空。BERT不是本文重点，略去介绍。

![](media/6164fe6ccd0b858bd28f9e6db82c610895100cede21dd9a387c27009024bc16f.jpg)  
图1-5 Transformer编码器和解码器的可视化展示。左侧的编码器部分展示了专注于掩码预测的类BERT大语言模型，主要用于文本分类等任务。右侧的解码器部分展示了类GPT大语言模型，主要用于生成任务和生成文本序列

# 如何降低推理成本

如何有效的降低推理成本，让LLM飞入寻常百姓家，是业界在积极探索的方向：

1. 硬件选型与架构创新

- 选用统一内存的硬件 硬件成本极低，能以较低价格获得巨大“显存”，轻松容纳百亿甚至千亿参数模型；但内存带宽成为性能瓶颈，相比顶级专用GPU，推理速度较慢 截至目前（2025年7月19日）可选：

- Mac Studio（约 4.5w¥）：统一内存的老前辈，内存带宽可达819GB/s（Ultra。Max 只有 410GB/s）- 作为参考，NVIDIA RTX 5090 显存带宽为 1792 GB/s- AMD Ryzen AI Max+ 395（约 1.5w¥）：128G 统一内存，可分配 96GB 给GPU
- 优点：同等显存下，本世代，硬件成本最低
- 缺陷（不是缺点）：内存带宽只有 256 GB/s- 即将上市：NVIDIA DGX Spark（约 2.9w¥，买两台有优惠）
- 128GB LPDDR5x 统一内存（带宽 273 GB/s）
- NVLink-C2C，900GB/s
- 网卡 ConnectX-7

异构计算

不强求模型完全载入显存，将 GPU/CPU、显存/内存/SSD 看作一个分级的计算/存储系统，智能调度数据

- 优点：配置门槛极低，理论上仅需少量内存和磁盘就能“跑起来”任意大的模型，当然，这种情况也仅仅只是能运行而已。- 缺点：性能是巨大挑战。频繁的数据交换（尤其涉及 SSD）会带来极高的延迟。能跑和能用还是有点区别

![](media/ff52c671b663acdb8cca2af6c467be80607d3c0042d2711add87aa34f7f09094.jpg)  
Bandwidth vs Capacity Across Devices

上图为不同硬件内存（Memory）带宽与容量的关系

幻方Vachel：LLM推理是内存IO限制，而不是计算限制。换句话说，目前加载1MB的数据到GPU所需的时间比1MB的数据在GPU上计算所需的时间长。这意味着LLM推理的吞吐量很大程度上取决于您能将多少批数据装入到高速GPU内存中。可选实现：

- Jittor/JittorLLMs：实现了张量在“显存-内存-硬盘”之间的自动交换。当GPU需要某层权重时，系统将其从内存或硬盘调入显存；计算完毕后，再将其换出。- ktransformers：专注于GPU/CPU混合推理。它会将计算密集型部分（如注意力机制）放在GPU上，而将其他部分或权重存储放在CPU和系统内存中。
- 最新版本要求AMX指令集，对Intel AMX指令集优化，利用现代CPU的AI加速能力来辅助GPU
- 好友实测AMX吞吐约1024ops/cycle，远不如插卡- 卡海战术：放弃昂贵的本代旗舰，转而使用大量便宜的、上代旗舰或消费级显卡，通过并行策略共同承载大模型

一张TeslaV100（16G，\~600￥）的算力，远不如RTX

5090（32G，\~20000￥）。但将RTX5090卖掉，就可以获得约30张V100（显存共480G）。通过NVLink可将6张V100串联，提供96GB显存，但满载功耗高达1.8千瓦，且核心内的HBM2显存因年代久远极易故障。

参考DeepSeek- R1671B满血版Q4量化模型，权重文件大小398G，还能剩下一些显存放KVCache。这是单5090不可能跑起来的模型。当然，这是一个比较理想化的例子：硬件上存在限制，需要一个能插如此多显卡的主板，并且进行高速的卡间互联（如NVLink），否则推理效率仍然不高。对于NVLink，NVIDIA RTX3090是最后一代支持NVLink的消费级游戏显卡，市面上有很多便宜的矿渣，还有大量改显存的版本。

优点：无与伦比的性价比，个人进行大语言模型的微调和训练不再是问题

缺点：对系统集成能力要求高（插槽、供电、散热），且卡间通信（特别是没有NVLink的卡只能走PCIe总线进行通信）会成为瓶颈，影响推理速度。需要配合张量并行和层间并行策略

2. 让模型的显存占用变得更小

- 量化。见上文，不再赘述。- 优化KVCache。生成式推理中，KVCache（后面有章节专门讲）的尺寸会随着生成序列的增长而线性增大，是显存消耗大户。

- 蒸馏（distill）：移除模型中冗余或不重要的权重/神经元，得到一个更稀疏、更小的模型。常见的做法是：训练一个参数量小得多的“学生模型”，让它学习并模仿一个强大的“教师模型”的行为，从而以更低的成本达到接近的效果。比如deepseek-ai/DeepSeek-R1-0528-Qwen3-8B：将DeepSeek-R1-0528（685B）的推理链蒸馏到Qwen3-8B，超越Qwen3-8B $10.0\%$ ，并达到了Qwen3-235B-thinking的性能水平。

- 使用高质量而不是大量的数据进行训练

Microsoft/Phi是微软推出的小型LLM，Phi- 3只有2.2GB，更易于在边缘场景（手机等）进行部署。

Phi系列模型的核心思想：Quality（质量） $>$ Quantity（数量），挑战了过去几年由GPT系列引领的ScalingLaw，旨在证明，通过使用"教科书"级别的高质量数据进行训练，一个小规模的大语言模型也能学到强大的推理和知识能力，达到甚至超越比它大几十倍的模型。用大白话来说，GPT系列模型是博古通今的智者，但可能受到训练数据影响，逻辑可能不严谨；而Phi是一个只读了从小学到大学所有人教版教材和五三习题集的学霸。

3. 提高硬件利用率以及算法创新

- 动态批处理（ContinuousBatching）

传统的静态批处理必须等待一个批次的所有请求都完成后才能处理下一个。动态批处理则持续地将新来的请求加入到当前批次中，一旦批次中任何一个请求完成，就立刻将其结果返回并填入新请求。该技术可以极大地提高GPU的吞吐量，让GPU始终处于忙碌状态，是目前所有高性能推理框架（如vLLM）的标配。

- 投机性解码（SpeculativeDecoding）

用一个非常小的、很快的模型（草稿模型）一次性生成多个候选词，然后用原始的大模型（验证模型）一次性并行验证这些词是否正确。如果小模型猜对了，就相当于用小模型的成本完成了大模型多次推理的工作量，将推理速度提升2- 3倍。

# KVCache

KVCache是一种在模型进行自回归推理时，缓存并复用过去计算出的 $K,V$ 向量的推理优化手段。它将每个生成步骤的计算复杂度从依赖于整个序列长度的平方级 $O(n^2)$ ，降低到了线性级 $O(n)$ ，从而极大地加快了生成速度。它是现代大语言模型能够实现快速推理的基石，但其代价是需要消耗额外的内存（VRAM/RAM/SSD等）资源来存储这些缓存数据。

注意，原始的Transformer架构中是没有KVCache的，这是Transformer推理时的优化手段。经典的KVCache是存在显存里的。

# 要解决的问题

自回归生成存在重复计算。Transformer 模型，尤其是像 GPT 这样的解码器（Decoder- only）模型，其生成文本的方式是自回归的。这意味着模型在生成第 $N$ 个 token 时，需要依赖前面已经生成的所有 $N - 1$ 个词作为上下文。这个依赖关系主要体现在自注意力（Self- Attention）机制中。我们回顾一下 Self- Attention 的计算过程：

$\bullet$ 输入的每个token，都会通过线性变换生成三个向量 $Q,K,V$ $\bullet$ 要计算某个token的注意力输出，需要用这个词的 $Q$ ，和所有词（包括自己）的 $K$ 进行点积计算，得到注意力分数 $\bullet$ 这些分数经过softmax归一化后，再与所有词的 $V$ 向量进行加权求和，得到最终的输出现在，我们来看看在生成文本时会发生什么。不妨假设输入是：“你好"，tokens $\equiv$ ["你"，"好"]，分别记作 $t_1$ 和 $t_2$

1. 现在要生成第三个token： $t_3$

$\bullet$ 为"你"和"好"分别计算 $Q_{1},K_{1},V_{1}$ 和 $Q_{2},K_{2},V_{2}$ $\bullet$ 为了预测下一个词，模型需要计算注意力O"你"的 $Q_{1}$ 需要和 $K_{1},K_{2}$ 计算注意力O"好"的 $Q_{2}$ 需要和 $K_{1},K_{2}$ 计算注意力 $\bullet$ 假设模型选择了 $t_3 =$ 世

2. 现在要生成第四个token： $t_4$

$\bullet$ 为"你"，"好"，"世"分别计算 $Q_{1},K_{1},V_{1},Q_{2},K_{2},V_{2}$ 和 $Q_{3},K_{3},V_{3}$ $\bullet$ 为了预测下一个词，模型需要计算注意力O"你"的 $Q_{1}$ 需要和 $K_{1},K_{2},K_{3}$ 计算注意力O"好"的 $Q_{2}$ 需要和 $K_{1},K_{2},K_{3}$ 计算注意力O"世"的 $Q_{3}$ 需要和 $K_{1},K_{2},K_{3}$ 计算注意力

发现问题了吗？在第2步中，我们重新计算了 $t_1,t_2$ 的 $K,V$ 向量！事实上，对于一个已经确定的词，它的 $K,V$ 向量是固定不变的。每生成一个新词，我们都要把之前所有词的 $K,V$ 重新计算一遍，这造成了巨大的计算浪费，尤其是在生成长文本时，序列长度不断增加，计算量会以平方级别增长。

# KVCache如何解决此问题

KVCache的核心思想非常简单：既然过去的 $K,V$ 向量是固定不变的，那就把它们缓存起来。

工作流程如下：

$\bullet$ 预填充阶段（Prefill）模型一次性处理整个输入，计算出提示中所有词（ $t_1 =$ 你， $t_2 =$ 好）的 $K,V$ 向量，分别记作 $\left(K_{1},V_{1}\right),\left(K_{2},V_{2}\right)$ ，然后把这些pairs存进cache里

$\bullet$ 解码/生成阶段（Decoding），目标是生成第3个token $t_3$

$\bigcirc$ 经过一次前向计算（forward），得到 $Q_{3},K_{3},V_{3}$ $\bigcirc$ 在进行注意力计算时，取出缓存中的 $\left(K_{1},V_{1}\right),\left(K_{2},V_{2}\right)$ $\bigcirc$ 将 $\left(K_{3},V_{3}\right)$ 与缓存中的 $K,V$ 进行拼接，得到完整的 $\left[K_{1},K_{2},K_{3}\right]_{\sqcup}$ 和 $\left[V_{1},V_{2},V_{3}\right]$ ，用 $Q_{3}$ 与完整的 $\left[K_{1},K_{2},K_{3}\right]$ 进行注意力计算 $\bigcirc$ 更新缓存，把 $\left(K_{3},V_{3}\right)$ 添加到cache的末尾 $\bigcirc$ 上述过程不断重复，直到出现生成结束符或达到最大长度。

题外话。关于上面提到的PD过程：当Prefill和Decode在同一块GPU上运行时，由于两阶段的计算特性差异（Prefill是计算密集型，而Decode是存储密集型），资源争抢会导致TTFT（Time- To- First- Token，首token的生成时间）和TPOT（Time- Per- Output- Token，生成每个token的时间）之间需要权衡。这是PD分离架构的动机。

KVCache的优点：

$\bullet$ 有效降低算法复杂度：这是指数级的性能提升，使得实时对话和长文生成成为可能 $\bullet$ 计算量显著减少：避免了大量的余矩阵乘法，节省了计算资源

那么，古尔丹，代价是什么呢：

$\bullet$ 占用额外内存：KVCache需要在显存中开辟一块空间来存储所有过去词的K，V向量。

# Memory Usage

2\*precision\*nlayers\*dmodel\*seqlen\*batch

$2 =$ twomatricesforKandV precision $=$ bytesperparameter(eg:4forfp32) nlayers $=$ layers in the model dmodel $=$ dimension of embeddings seglen $=$ length of context in tokens batch $=$ batch size

上图为KVCache内存占用的计算方式。实际上我们也不需要自己算，网上有很多计算器

$\bullet$ 当序列很长时（例如处理几十万token的上下文，如下一代RAG架构Index- freeRAG场景），KVCache会占用非常大的显存，这也是为什么长上下文模型对显存要求极高的主要原因之一。如果解决不了此问题，Index- freeRAG只能用小模型进行计算。

为了支持更长的上下文，可以做的工作有：

$\bullet$ KVcacheoffload。VRAM不足时，将一部分不那么重要的 $(K,V)$ 从高速的VRAMoffload到低速但容量大的RAM，甚至更慢的NVMeSSD。当需要它们时，再取回到VRAM。

相关项目或产品有LMCache（Redis for LLMs）、宏杉科技MC27000（带宽135G/s）。关于分层存储，几个可关注的点：

简单的LRU（Least Recently Used）卸载策略在需要访问久远信息时，由于PCIe带宽瓶颈（期待PCIe 7.0）的存在会产生巨大的延迟（不知道上次PengLian提到的S3- FIFO表现如何）可以设计更精细的计算/通信流水线，让KVCache offload/fetch与计算过程完全并行，隐藏数据搬运的延迟卸载时进行 $f16\rightarrow$ int的量化

$\bullet$ 评估KVcache pair的重要性，可以只保留有用的部分，淘汰不重要的部分。比如可以追踪每个历史Token被"注意"的频率或强度，持续淘汰那些注意力得分长期偏低的KV- pairs。 $\bullet$ 彻底摆脱注意力机制，比如Mamba

# 稠密模型、MoE架构以及DeepSeek的精髓

稠密模型（Dense model）是传统意义上的大语言模型，如早期的GPT- 3等，对于你输入的每一个词（Token），模型中的所有参数都会参与计算。想象有一个庞大的专家委员会（比如8B模型有80亿个w，参见上文），无论问题是关于天文、地理还是数学，所有专家都必须坐在一起开会，共同讨论并给出最终答案。

稠密模型的优点在于：结构简单，易于训练，性能强大（遵循ScalingLaw，数据越多越厉害）

稠密模型的缺点：计算成本极高。随着模型参数增加到千亿级别，每一次训练和推理都需要调动海量的计算资源，带来了成本的暴涨：

$\bullet$ 训练成本：GPT- 4的训练成本超过一亿美元 $\bullet$ 推理成本：用户每次提问的成本高，响应速度慢

很显然，稠密模型在如今的参数量和硬件限制下，已经达到了难以为续的程度。

MoE，Mixture- of- Experts，混合专家架构，最早由Hinton等人提出- (哈哈，又是他)，在Mixtral和DeepSeek等开源模型中大放异彩。MoE模型不再让所有参数参与计算，而是包含大量的"专家"子网络。对于每一个输入token，门控网络（GatingNetwork）会智能地判断，并将这个任务只分配给最相关的少数几个专家（通常是2\~4个）。

MoE架构的优点：总参数量巨大，但计算成本极低。一个模型可以拥有上万亿的总参数，但每次推理时，只激活其中一小部分（例如，一个236B的MoE模型，可能每次只激活21B参数）。这使得训练和推理速度大幅提升，成本显著降低。

MoE架构的主要挑战（可参考华为CloudMatrix384论文）：

$\bullet$ 训练复杂性与不稳定性

MoE的训练面临独特的挑战。其门控网络（GatingNetwork）在初期倾向于将数据路由到少数几个表现优异的"明星专家"，导致其他专家得不到充分训练。为避免这种"赢者通吃"的现象，必须引入精心设计的负载均衡损失函数（LoadBalancingLoss），以强制实现路由的均衡，但这无疑增加了训练的复杂度和调优难度。

$\bullet$ 高昂的显存和通信开销：

MoE架构存在一个核心矛盾：计算是稀疏的，但存储是密集的。在推理时，虽然只有一小部分专家被激活和计算，但所有专家的参数都必须完整加载到显存中。这使得MoE模型对硬件的显存容量要求极高，成为了部署和应用的一大瓶颈。在分布式训练/推理中，不同专家可能分布在不同GPU上，门控网络和专家之间的数据传输会带来额外的通信负担。

然而，上述问题瓶颈也催生了新的技术方向。MoE架构的特性为构建跨VRAM、RAM和SSD的智能调度系统提供了可能（仅为个人见解）：

$\bullet$ 分层基础：模型巨大的参数量和专家使用频率的差异，天然形成了"热数据”（如门控网络、明星专家）和"冷数据”（不常用专家）的分界，这是实现数据分层管理的基础。 $\bullet$ 智能预加载：在推理请求并发的场景下，调度系统可以利用门控网络的路由预测能力，提前将即将被激活的专家从低速存储（RAM/SSD）加载到VRAM，从而在不牺牲太多性能的前提下，大幅降低对VRAM需求。 $\bullet$ 高效扩缩容：在服务扩容时，系统无需一次性加载全部模型权重。它可以优先加载核心的"明星专家"以快速启动服务，后台异步加载其余专家，实现平滑、高效的弹性伸缩。

下一个要讲的明星是DeepSeek，其特点在于便宜（包括训练和推理）和开放。

以DeepSeek- V2为例，它是一个拥有2360亿（236B）总参数的MoE模型。但其巧妙之处在于：

$\bullet$ 共享专家：设置一部分专家作为"共享专家"，这些专家总是被激活，用于捕捉和整合常见的跨上下文知识。这样可以减少专家之间的知识余，每个专家可以更专注于独特的知识领域。 $\bullet$ 细粒度路由：每次推理只激活210亿（21B）参数。这意味着它的推理成本和速度，大致相当于一个21B的稠密模型，但其能力却源自一个庞大的236B知识库。

对于推理时的显存占用（KVCache）仍然是一个巨大瓶颈。DeepSeek- V2引入了多头隐注意力（Multi- headLatentAttentionMLA）。MLA的作用：传统注意力机制中，KVCache的大小与上下文长度成正比，导致长文本推理非常耗费显存。MLA通过一个压缩的隐向量来缓存关键信息，将KVCache的大小减少了几个数量级，从而大幅降低了长文本推理的显存占用，显著提升了推理吞吐量。

与GoseAIOpenAlGPT- 4等模型不同，DeepSeek选择了开放权重、推理代码等（没有开放训练代码，但官方允许微调、蒸馏），将性能比肩顶尖闭源模型的MoE模型开放给社区，极大地降低

了研究者和开发者使用尖端 AI 技术的门槛。伟大，无需多言！

# 引用

累了，晚点再写引用吧。7.21 留，感觉也不用写了，都是链接。

- The history of artificial intelligence- 动图轻松理解 Self-Attention（自注意力机制）
