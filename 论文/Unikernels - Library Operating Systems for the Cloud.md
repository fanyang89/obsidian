我们提出了单内核系统（unikernels），这是一种通过用高级源代码编写的应用程序来部署云服务的新方法。单内核系统是单用途的设备，在编译时专门化为独立的内核，并在部署到云平台时被密封以防止修改。作为回报，它们显著减小了镜像大小，提高了效率和安全性，并应降低运营成本。我们的 Mirage 原型将 OCaml 代码编译成在商用云上运行的单内核系统，在不产生显著性能损失的情况下将代码大小减小了一个数量级。该架构将静态类型安全与单一地址空间布局相结合，该布局可以通过虚拟机管理程序扩展变为不可变的。Mirage 贡献了一套类型安全的协议库，我们的结果表明，虚拟机管理程序是一个平台，它克服了硬件兼容性问题，这些问题使得过去的库操作系统在实际部署中不切实际。

# 引言

操作系统虚拟化通过提供一个平台，让客户租用资源来托管虚拟机（VM），彻底改变了大规模计算的经济模式。每个虚拟机都表现为一台独立的计算机，启动标准的操作系统内核并运行未修改的应用程序进程。每个虚拟机通常专门用于特定的角色，例如数据库、Web 服务器，而向外扩展则涉及从模板镜像克隆虚拟机。
尽管从在多用户操作系统上运行的应用程序转变为提供许多单用途虚拟机实例，但部署到云端的镜像中几乎没有实际的专门化。我们在专门化方面采取极端立场，通过在编译时去除功能，将最终的虚拟机镜像视为单用途设备而非通用系统。具体而言，我们的贡献如下：（i）提供密封单用途设备的unikernel方法，特别适用于提供云服务；（ii）使用函数式编程语言（OCaml）对这些技术的完整实现进行评估，表明类型安全的好处不会损害性能；（iii）支持在OCaml中进行系统编程的库和语言扩展。

![](https://fuis.me/static/img/b1a14a39a3db8359dc0fa4d8178113cc.clipboard-2024-12-23.webp)

unikernel 方法建立在过去对库操作系统的研究工作之上[1-3]。整个软件栈包括系统库、语言运行时和应用程序被编译成一个可直接在标准虚拟机管理程序上运行的单一可启动虚拟机镜像（图 1）。通过针对标准虚拟机管理程序，unikernel 避免了传统库操作系统（如 Exokernel [1]和 Nemesis [2]）遇到的硬件兼容性问题。与 Drawbridge [3]不同，通过避开向后兼容性，unikernel 面向云服务而非桌面应用程序。通过使用库操作系统针对商品云，与 Singularity [4]相比，unikernel 可以提供更高的性能和更好的安全性。最后，与 Libra [5]不同，Libra 在 Xen 上为 JVM 提供一个 libOS 抽象，但依赖于一个单独的 Linux 虚拟机实例来提供网络和存储，unikernel 是更加高度专业化的单用途设备虚拟机，可直接集成通信协议。
我们以基于 OCaml 的 Mirage 实现形式描述了一个完整的单内核原型（§3）。我们通过微基准测试和提供 DNS、OpenFlow 和 HTTP 的设备对其进行评估（§4）。我们发现牺牲源代码级别的向后兼容性可以提高性能，同时显著提高面向外部的云服务的安全性。我们通过标准网络协议（如 TCP/IP）保持与外部系统的兼容性，而不是尝试支持 POSIX 或其他用于应用程序构建的常规标准。例如，Mirage DNS 服务器的性能优于 BIND 9（高出 45%）和高性能 NSD 服务器（§4.2），同时使用非常小的虚拟机镜像：我们的单内核设备镜像仅为 200 kB，而 BIND 设备则超过 400 MB。我们通过讨论构建 Mirage 的经验及其在现有技术中的地位来得出结论（§5），并进行总结（§6）。

# 架构

虚拟化是云计算的支持技术，通过 Xen 等虚拟机管理程序广泛部署[6]。虚拟机设备旨在提供一小组固定的服务。因此，数据中心设备通常由在 Xen 上引导的 Linux 或 Windows 虚拟机组成，具有松散耦合的组件：一个承载主要应用程序（例如 MySQL、Apache）的客户机操作系统内核，其他服务（例如 cron、NTP）并行运行；并且通常连接带有配置文件和数据的外部存储设备。
我们的关键见解是，虚拟机管理程序提供了一种可以动态扩展的虚拟硬件抽象——既可以通过添加内存和虚拟CPU进行纵向扩展，也可以通过生成更多虚拟机进行横向扩展。这为库操作系统（libOS）提供了一个绝佳的目标，库操作系统是一个古老的想法[1,2]，最近被重新审视用于拆分单体操作系统[3]。由于难以支持足够广泛的现实世界硬件，库操作系统从未得到广泛部署，但在诸如Xen这样的虚拟机管理程序上进行部署，使我们能够通过使用虚拟机管理程序的设备驱动程序来绕过这个问题，从而有机会构建一个在云计算基础设施上原生运行的实用的全新库操作系统。
我们将这些虚拟机称为单内核：专门的、密封的、单一用途的库操作系统虚拟机，直接在管理程序上运行。库操作系统的结构与传统操作系统非常不同：从调度程序到设备驱动程序再到网络堆栈的所有服务都被实现为直接与应用程序链接的库。结合选择一种现代的静态类型安全语言进行实现，这为单内核提供了配置、性能和安全优势。

## 配置和部署

配置在管理大型云托管服务的部署方面是一个相当大的开销。尽管在 Linux 上有（多个）关于配置文件位置和格式的标准，并且 Windows 有注册表和活动目录，但在应用程序配置的许多方面没有标准。为了解决这个问题，例如，Linux 发行版通常会采用大量的 shell 脚本来将软件包组合在一起。
单内核（Unikernels）采用了一种不同的方法，即将配置集成到编译过程中。单内核不是将数据库、网络服务器等视为必须通过配置文件连接在一起的独立应用程序，而是将它们视为单个应用程序中的库，允许应用程序开发人员使用简单的库调用来配置动态参数，或者使用构建系统工具来配置静态参数。这具有使配置决策在宿主语言中明确且可编程的有用效果，而不是操作许多临时文本文件，因此可以受益于静态分析工具和编译器的类型检查器。最终结果是大大减少了配置复杂的多服务应用程序虚拟机所需的工作量。

## 2.2 Compactness and Optimisation

云中的资源是租用的，最大限度地减少其使用可降低成本。同时，多租户服务在负载方面具有高度的可变性，这促使人们快速扩展部署以满足当前需求而不浪费资金。单内核将通常由主机操作系统提供的库链接起来，允许单内核工具通过正常的链接机制生成高度紧凑的二进制文件。在特定编译中未使用的功能不会被包含，并且可以使用全系统优化技术。在最特殊的模式下，所有配置文件都进行静态评估，从而能够以必须重新编译才能重新配置服务为代价实现广泛的死代码消除。较小的二进制文件大小（在许多情况下约为千字节级别）使得通过互联网部署到远程数据中心更加顺畅。

## 2.3 Unikernel Threat Model and Implications

在考虑单内核抽象的安全影响之前，我们首先说明我们的背景和威胁模型。我们关注的是在多租户数据中心中提供面向网络服务的软件。云提供商的客户通常必须信任提供商不会有恶意行为。然而，在这样的环境中运行的软件始终面临着来自其他租户以及更广泛的互联网连接主机的攻击威胁。
单内核（Unikernels）在管理程序层之上运行，并将其和控制域视为可信计算基础的一部分（目前，请参阅§5.3）。然而，单内核并没有采用对于专用设备来说过于复杂的多用户访问控制机制，而是将管理程序作为唯一的隔离单元，并让应用程序通过诸如 SSL 或 SSH 等协议库信任外部实体。在内部，单内核采用深度防御方法：首先通过编译时的专门化，然后通过运行代码中的普遍类型安全，最后通过管理程序和工具链扩展来防范不可预见的编译器或运行时错误。

### 2.3.1 Single Image Appliances

现有的应用程序通常需要向后兼容，例如 POSIX API、操作系统内核以及众多涉及的用户空间二进制文件，这意味着即使是最简单的设备虚拟机也包含数十万（如果不是数百万）行活动代码，每次启动时都必须执行这些代码（§4.5）。即使是像 Samba 和 OpenSSL 这样广泛部署的代码库，在 2012 年 4 月仍存在远程代码执行漏洞[7,8]，并且严重的数据泄露在现代互联网服务中已变得极为常见。一个特别隐蔽的问题是，错误配置镜像可能会使不必要的服务运行，这会大大增加远程攻击面。
一种单内核工具链尽可能多地执行编译时工作，以从最终的虚拟机中消除不必要的功能。所有网络服务都作为库提供，因此只有在配置文件中明确引用的模块才会被链接到输出中。模块依赖图可以很容易地进行静态验证，以确保只包含所需的服务。虽然有一些 Linux 包管理器采用了这种方法[9]，但它们最终受到必须支持动态 POSIX 应用程序的限制。
使用过多编译到镜像中的静态配置指令的权衡是，虚拟机不能再通过对现有镜像进行写时复制快照来克隆。如果需要这样做，可以使用动态配置指令（例如，使用 DHCP 而不是静态 IP）。我们的 Mirage 单内核原型包含的代码行数比等效的 Linux 少得多，并且生成的镜像明显更小（§4.5）。

### 2.3.2 Pervasive Type-Safety

对远程攻击具有强大的抵御能力这一要求强烈促使人们使用类型安全的语言。一个重要的决策是是否在同一个单内核系统中支持多种语言。支持多种语言的一个理由是提高与现有代码的向后兼容性，但代价是增加了单镜像系统的复杂性以及需要处理多种语言运行时之间的互操作性。
另一种选择是避开源代码级别的兼容性，完全用一种语言重写系统组件，并尽可能优化该工具链。虽然重写像 TCP 这样的协议是一项艰巨的工程挑战，但对于像我们的 Mirage 原型这样的实验系统来说是可能的。选择这条道路，我们在网络协议级别支持互操作性：组件使用标准网络协议的类型安全、高效实现进行通信。在管理程序上运行的优势在于反过来也是可能的：现有的非 OCaml 代码可以封装在单独的虚拟机中通过消息传递进行通信，类似于传统操作系统中的进程（§5.2）。同样，设备内的访问控制不再需要用户空间进程，而是依靠语言的类型安全来实施限制。虚拟地址空间可以简化为单地址空间模型。
Mirage 的单一语言专注性简化了安全技术的集成，以保护系统中剩余的非类型安全组件（特别是垃圾收集器），并在编译器错误导致类型安全属性被违反的情况下提供深度防御。其中一些技术，例如栈金丝雀和保护页，是标准技术的直接转化，因此我们不再进一步讨论它们。然而，有两种技术依赖于单内核环境的独特属性，我们接下来将对其进行描述。

### 2.3.3 Sealing and VM Privilege Dropping

由于单内核操作系统是单镜像和单地址空间，它们对虚拟机接口的考验较少，并且可以在运行时进行密封[10]，以进一步防范运行时或编译器中的错误。这意味着在编译时未出现在单内核操作系统中的任何代码都永远不会被运行，完全防止了代码注入攻击。实现此策略非常简单：作为其每日初始化的一部分，单内核操作系统建立一组页表，其中没有一页既是可写的又是可执行的，然后发出特殊的密封超级调用，以防止进一步的页表修改。在虚拟机被密封时生效的内存访问策略将一直保留，直到它终止。实现密封操作所需的虚拟机管理程序更改本身非常简单；1 相比之下，在传统操作系统中实现等效的“写异或执行”[11]策略需要对库、运行时和操作系统内核本身进行大量修改。
这种方法确实意味着正在运行的虚拟机无法扩展其堆，而是必须在启动时预先分配其所需的所有内存（堆内的分配不受影响，并且管理程序仍然可以在虚拟机之间过度分配内存）。对于云基础架构来说，这是一个合理的限制，因为分配给虚拟机的内存已经被购买。禁止修改页表不适用于 I/O 映射，前提是它们本身不可执行并且不替换任何现有数据、代码或保护页。这意味着 I/O 不受密封虚拟机的影响，并且不会从本质上使内存访问策略无效。
这种可选的设施是单内核系统中唯一需要对管理程序进行修补而不是纯粹在客户机中运行的元素。权限降低补丁非常简单，并且会使任何其他单地址空间操作系统受益，因此它正在被上游合并到主要的 Xen 发行版中。请注意，Mirage 可以在未经修改的 Xen 版本上运行而无需此补丁，尽管会失去这一层深度防御安全保护。

### Compile-Time Address Space Randomization

虽然虚拟机密封可以防止攻击者引入攻击代码，但它无法阻止攻击者执行已经存在的代码。尽管使用全系统优化可以消除许多此类攻击的目标，但可能仍有足够的目标可用于使用面向返回的编程风格技术来组装可行的漏洞利用。按照惯例，这些可以通过运行时地址空间随机化来防范，但这需要运行时链接器代码，这会给正在运行的单内核引入显著的复杂性。幸运的是，这也是不必要的。单内核模型意味着重新配置设备意味着重新编译它，可能每次部署都需要重新编译。因此，我们可以在编译时使用新生成的链接器脚本执行地址空间随机化，而不会阻碍任何编译器优化，也不会增加任何运行时复杂性。

# Mirage Unikernels

我们的 Mirage 原型通过将 OCaml 代码编译并链接到可引导的 Xen VM 映像中来生成单内核。我们在 OCaml 中实现除了最低级别功能之外的所有功能，并且为了帮助开发人员测试和调试他们的代码，提供了生成可在 UNIX 上运行 Mirage 服务的 POSIX 二进制文件以及 Xen VM 映像的能力。现在我们讨论 Mirage 的一些关键设计决策和组件：OCaml 的使用（§3.1）、初始化基本环境的 PVBoot 库（§3.2）、用于堆管理和并发的修改后的语言运行时库（§3.3）以及其类型安全的设备驱动程序（§3.4）和 I/O 栈（§3.5）。

## 3.1 Why OCaml?

我们选择用 OCaml 实现 Mirage 有四个关键原因。首先，OCaml 是一种成熟的系统编程语言[12]，具有灵活的编程模型，支持函数式、命令式和面向对象编程，其简洁性减少了代码行数（LoC），而代码行数通常被认为与攻击面相关。其次，OCaml 有一个简单而高性能的运行时，使其成为试验与 Xen 交互的单内核抽象的理想平台。第三，它对静态类型的实现消除了编译时的类型信息，同时保留了类型安全的所有好处，这是专业化的另一个例子。最后，开源的 Xen Cloud Platform[12]和关键系统组件[13,14]是用 OCaml 实现的，使得集成变得简单直接。
然而，这种选择确实带来了权衡。与其他系统语言（如 C/C++）相比，OCaml 仍然是一种相对深奥的语言。使用 OCaml 还需要大量的工程努力来重建系统组件，特别是存储和网络栈。考虑到 OCaml 的其他好处，我们认为这两者对于研究原型来说都不是重大障碍。我们早期做出的一个决定是采用多核理念，即每个核心运行一个虚拟机，而单线程的 OCaml 运行时具有快速的顺序性能，非常适合这种需求。每个 Mirage 单内核在 Xen 上运行，使用一个虚拟 CPU，并且通过在单个 Xen 实例上的多个通信单内核来支持多核。
我们确实探索了在传统的系统语言 C 中应用单内核技术，将应用程序代码与 Xen MiniOS（一个精简的 libc、OpenBSD 版本的 libm 和 printf 以及 lwIP 用户空间网络栈）链接起来。然而，我们发现，以这种方式从高性能 NSD DNS 服务器构建的 DNS 设备的性能比 Mirage DNS 服务器差很多，即使经过几轮优化（图 10）。似乎用 C 语言生成一个性能类似的原型仍然需要非常大的工程努力，并且不会获得任何类型安全方面的好处。

## PVBoot Library

PVBoot 提供每日启动支持，以初始化具有一个虚拟 CPU 和 Xen 事件通道的虚拟机，并跳转到入口函数。与传统操作系统不同，它不支持多进程和抢占式线程，而是为语言运行时布置了一个单一的 64 位地址空间。PVBoot 提供两种内存页分配器，一种是 slab 分配器，一种是 extent 分配器。slab 分配器用于支持运行时中的 C 代码；由于大多数代码是用 OCaml 编写的，所以它没有被大量使用。extent 分配器保留一个连续的虚拟内存区域，它以 2MB 的块进行操作，允许映射 x86 64 超级页。内存区域被静态地分配角色，例如垃圾回收堆或 I/O 数据页。PVBoot 还有一个 domainpoll 函数，它在一组事件通道和超时时间上阻塞虚拟机。

![](https://fuis.me/static/img/a41813c63d38835b99c7df72f0fcdd8e.clipboard-2024-12-23.webp)

PVBoot 为异步、事件驱动的虚拟机提供了最低限度的支持，该虚拟机在 I/O 可用或超时之前一直处于休眠状态。设备驱动程序都在 PVBoot 之外以类型安全的 OCaml（§3.4）提供。

## 语言运行时

Mirage 在经过专门修改的语言运行时上执行 OCaml 代码，主要在两个关键领域进行了修改：内存管理和并发。图 2 展示了 Mirage 单内核的内存布局，分为三个区域：文本和数据；外部 I/O 页面；以及 OCaml 堆。文本和数据段包含针对 PVBoot 链接的 OCaml 运行时。这是内核中唯一可用的地址空间。应用程序的主线程在启动后立即启动，当它返回时，虚拟机关闭。
OCaml 垃圾回收器将堆分为两个区域：一个用于短生命周期值的快速小堆，以及一个在每次小堆收集时将长生命周期值提升到其中的大型主堆。这些区域被分配在 Xen 预留的低虚拟地址空间之下：小堆有一个单一的 2MB 范围，以 4KB 块增长，主堆拥有虚拟内存的剩余部分，使用范围分配器以 2MB 的超级页块增长。通常不鼓励内存映射大的连续区域以允许地址空间随机化（ASR）来防止缓冲区溢出[16]，因此普通的用户空间垃圾回收器维护一个页表来跟踪分配的堆块。Mirage 单内核在运行时避免 ASR，而倾向于更专门的安全模型（§2.3），并保证连续的虚拟地址空间，简化了运行时内存管理。
虚拟机通过让本地虚拟机通过管理程序向远程虚拟机授予内存页面访问权限来直接通信[17]。PVBoot 从虚拟内存的保留区域分配外部内存页面，并在小型、快速的 OCaml 小堆中分配一个小的代理值。Mirage 提供了一个库，可从 OCaml 引用该数据而无需数据复制（§3.4）。以这种方式专门化内存布局以区分 I/O 页面，可显著减少垃圾收集器必须扫描的数据量。这种降低的垃圾收集器压力是让 Mirage 网络堆栈表现出可预测性能的两个关键因素之一；另一个是对零拷贝 I/O 的普遍库支持（§3.4.1）。
为了提供超出 PVBoot 的简单 domainpoll 函数的并发性，Mirage 集成了 Lwt 协同线程库[18]。这在内部将阻塞函数评估为事件描述符，为开发人员提供直线控制流。Lwt 线程完全用 OCaml 编写，是堆分配的值，只有线程主循环需要一个 C 绑定来轮询外部 events. Mirage 提供了一个评估器，它使用 domainpoll 来监听事件并唤醒轻量级线程。因此，虚拟机要么正在执行 OCaml 代码，要么被阻塞，没有内部抢占或异步中断。主线程反复执行，直到它完成或抛出异常，随后域将关闭，虚拟机退出代码与线程返回值匹配。

![](https://fuis.me/static/img/9fb2ebd7e578118cb300ac358f0714eb.clipboard-2024-12-23.webp)

一个有用的结果是，大多数调度和线程逻辑包含在应用程序库中，因此开发人员可以根据自己的需要进行修改。例如，线程可以根据应用程序的需求用本地键进行标记，用于调试、统计或优先级排序。线程调度与平台无关，定时器存储在堆分配的 OCaml 优先级队列中，并且可以被应用程序覆盖（例如，我们之前在早期原型中展示了对 Sqlite 库数据库进行自定义调度的好处[19]）。只有运行循环是特定于 Xen 的，用于与 PVBoot 进行交互。

## 设备驱动程序

Mirage 驱动程序是对 Xen 提供的设备抽象的接口。Xen 设备由客户虚拟机中的前端驱动程序和后端驱动程序组成，后端驱动程序通常将前端请求多路复用到真正的物理设备上[20]。它们通过事件通道连接以向对方发出信号，并且有一个单一的内存页面被划分为固定大小的请求槽，由生产者/消费者指针跟踪。响应被写入与请求相同的槽中，前端实现流控制以避免使环溢出。使用此模型的 Xen 设备类别包括以太网、块存储、虚拟帧缓冲区、USB 和 PCI。
在 Mirage 中，对这些共享内存环的操作是所有 I/O 的基本抽象。共享页面使用内置的 Bigarray 模块映射到 OCaml 中，该模块将外部分配的内存安全地包装到 OCaml 堆中，并使其作为数组可用。在共享页面中进行读写必须与 C 语义精确匹配，并且在 OCaml 中这是一个相对较慢的操作，因为固定大小的整数是装箱值，在被复制到共享页面之前是在堆上分配的。我们使用 camlp4 为 OCaml 添加了一个新的 cstruct 关键字来直接映射 C 结构（图 3）。声明一个 cstruct 会生成用于直接操作外部内存数组的访问函数。该扩展还处理字节序转换，并在网络栈中广泛用于报头解析[22]。
这种方法允许 Mirage 驱动程序被实现为纯 OCaml 库，仅依赖于两个额外的内部函数：提供读和写内存屏障的内联汇编。Ring 模块实现基本协议，并添加一个异步线程接口来推送请求并等待响应。更高层次的模块，如 Netif 和 Blkif，实现网络和块驱动程序，并与未修改的 Xen 主机进行交互操作。我们重新实现这些协议的一个有益的副作用是对现有代码进行模糊测试，结果我们发现并报告了 Linux/Xen 中的几个边缘情况错误，包括一个重要的安全问题（XSA-39）。

![](https://fuis.me/static/img/f3331a63a7cb0789bc03279065fcb9ff.clipboard-2024-12-23.webp)

## 零拷贝设备 I/O

Xen 设备协议不是将数据直接写入共享内存页，而是使用它来协调通过引用传递 4kB 内存页。两个通信的虚拟机共享一个授权表，该表将页映射到该表中的整数偏移量（称为授权），更新由管理程序检查和执行。授权在设备驱动程序共享环上进行交换，并由远程虚拟机查找以将该页映射或复制到其自己的地址空间中。一旦在远程（非 Mirage）虚拟机中，数据必须传输到应用程序中。由于 POSIX API 不支持零拷贝套接字，这通常需要从接收虚拟机的内核进行第二次复制到适当的用户空间进程中。
Mirage unikernels 没有用户空间，因此接收到的页面会直接传递给应用程序，无需复制。cstruct 库通过将底层数组切成较小的视图来避免产生复制开销；一旦所有视图都被垃圾回收，数组将被返回到空闲页面池。因此，网络栈可以安全地重复使用传入网络流量的片段，并将其直接代理到另一个设备（例如，HTTP 代理），同时避免手动管理页面。
然而，仍有一些资源必须手动跟踪和释放，例如虚拟机之间共享授权表的内容。Mirage 使用 OCaml 类型系统来强制执行不变量，以确保通过包装给定资源（如授权引用）的任何使用的高阶函数正确释放资源。当函数终止时，无论是通过超时正常终止还是由于未知异常终止，授权引用都会被释放。这不仅可以防止资源泄漏，特别是在错误路径上，还允许调度程序通过取消轻量级线程来释放资源。这些可组合的高阶函数，也称为组合子，在 Mirage 中被广泛用于安全地暴露系统资源。请注意，这些组合子并不能完全消除资源泄漏，因为保存在数据结构中且从未被删除的引用将永远存在，但 OCaml 编程风格鼓励使用许多小型数据结构和可重用的实用程序库，这有助于防止此类问题。

## 类型安全的协议 I/O

Mirage 在 OCaml 中实现协议库，以确保所有外部输入/输出处理都是类型安全的，使单内核系统对内存溢出具有强大的抵抗力。协议被构建为具有单独的客户端/服务器逻辑的非阻塞解析库，该逻辑会生成轻量级线程。通过抢占式状态句柄访问库，从而在同一个单内核系统中实现多个协议栈。表 1 列出了目前在 Mirage 中实现的协议，足以在亚马逊 EC2 公共云上自托管我们的网站基础设施，包括维基、博客和 DNS 服务器。
![](https://fuis.me/static/img/9887eb2b9402a44f8d86ee84669fd7bf.clipboard-2024-12-23.webp)
数据以离散数据包流的形式到达网络和存储栈。Mirage 通过使用通道迭代器[23]弥合了数据包和流之间的差距，这些迭代器将函数映射到无限的数据包流上以产生类型化的流。迭代器消除了许多在耦合不那么紧密的传统内核/用户空间中经常使用的固定大小缓冲区。链式迭代器将流量直接路由到相关的应用线程，必要时在中间系统事件上阻塞。我们现在描述网络（§3.5.1）和存储（§3.5.2）的具体情况。

## 网络处理

Mirage网络栈强调应用程序级别的控制而非严格的模块化，将底层协议的大多数细节暴露给应用程序控制。它提供了两种通信方法：一种是快速的主机内虚拟机间的vchan传输，用于外部通信的以太网传输。vchan是一种快速共享内存互连，通过生产者/消费者指针来跟踪数据。它为环形缓冲区分配多个连续的页面，以确保有合理的缓冲区，并且一旦连接，通信的虚拟机可以通过共享内存直接交换数据，除了中断通知外，无需虚拟机管理程序的进一步干预。从Linux 3.3.0开始，vchan存在于上游版本中，使得Mirage unikernels和Linux虚拟机之间能够轻松交互。
互联网连接更加复杂：应用程序链接到诸如 HTTP 之类的协议库，该协议库与 TCP/IP 网络栈链接，而网络栈又与网络设备驱动程序链接。应用程序与协议库读写 I/O 页面，以便它们可以直接传输。在读取数据包时，网络栈使用 cstruct 子视图（§3.4.1）分离出头部和数据。写入数据需要更多处理，因为网络栈必须在数据有效载荷之前为 TCP、IP 和以太网添加可变长度的协议头部，并且有时会将大型有效载荷分割成较小的片段进行传输。这通过分散-收集 I/O 来解决：网络栈为每次写入分配一个头部页面，并且网络库根据需要将传入的有效载荷重新排列成子视图，然后将所有片段作为一个数据包写入设备环。图 4 说明了这个写入路径。

## 存储

传统操作系统内核在通过 POSIX 套接字或 mmap 访问的块设备上分层文件系统，并将写入合并到内核缓冲区缓存中[24]。需要精确控制写入实际持久化时间的应用程序必须调用 fsync 系统调用，或者显式请求非缓冲直接访问并发出扇区对齐的请求。现代 Linux 内核仅为异步块请求提供 libaio，这迫使应用程序在网络和存储方面使用不同的 API。
相比之下，Mirage块设备与网络设备共享相同的环形缓冲区抽象，使用相同的I/O页面来提供高效的块级访问，文件系统和缓存作为OCaml库提供。这使得应用程序可以控制缓存策略，而不是只提供一种默认的缓存策略。不同的缓存策略可以作为库（OCaml模块）在构建时链接，唯一的内置策略是保证所有写入都是直接的。
例如，我们将一个第三方写时复制二叉树存储库移植到了 Mirage。它可以被应用程序用作存储后端，缓存策略和缓冲区管理在库内被明确管理。我们的 FAT-32 存储库也实现了自己的缓冲区管理策略，其中数据读取以迭代器的形式返回，每次提供一个扇区。这避免了在堆中构建大型列表，同时通过让它从块驱动程序请求更大的扇区范围，允许库内进行内部缓冲。最后，我们发现我们的 DNS 服务器通过将记忆化库应用于网络响应（§4）获得了巨大的速度提升；这种技术也可以用于实现非常大的数据集的持久自分页[25]。

# 评估

由于 Mirage 是许多操作系统组件的全新实现，我们分阶段将其与更传统的部署进行评估。我们首先检查微基准测试（§4.1）以确定关键组件的基线性能，接着是更现实的设备：一个 DNS 服务器，展示我们安全网络堆栈的性能（§4.2）；一个 OpenFlow 控制器设备（§4.3）；以及一个集成的 Web 服务器和数据库，结合了存储和网络（§4.4）。最后，我们检查这些设备中有效代码行数和二进制文件的差异，以及死代码消除的影响（§4.5）。

## 微型基准测试

这些微基准测试通过检查简单、受控场景中的性能，展示了单内核专业化的潜在优势。评估由在不同托管环境中执行的相同 OCaml 代码组成：linux-native，直接在裸机上运行的 Linux 内核，带有应用程序的 ELF 二进制版本；linux-pv，作为半虚拟化 Xen domU 运行的 Linux 内核，带有应用程序的 ELF 二进制版本；以及 xen-direct，构建为类型安全的单内核的应用程序，直接在 Xen 上运行。我们验证了受 CPU 限制的应用程序如预期般不受单内核编译的影响，因为管理程序架构仅影响内存和 I/O。

## 启动时间

单内核系统足够紧凑，可以启动并实时响应网络流量。
Mirage生成的虚拟机紧凑且启动速度极快。图5对比了Mirage网络服务器、最小化Linux内核以及运行Apache2的更实际的linux - pv Debian Linux的启动时间。时间是从启动到启动完成的测量，以虚拟机向控制域发送一个特殊的UDP数据包作为完成标志。最小化Linux内核通过一个initrd来测量“进入用户空间的时间”，该initrd在显式发送单个UDP数据包之前直接调用ifconfig ioctl来启动网络接口。运行Apache2的更实际的Debian Linux使用标准的Debian启动脚本，并通过等待Apache2启动返回后再发送单个UDP数据包来测量进入用户空间的时间。Mirage unikernel在网络接口准备好后立即发送UDP数据包。随着内存大小增加，Mirage启动时间中用于构建域的比例也增加，在内存大小为3072 MiB时达到约60%。Mirage与最小化Linux内核相当，启动时间略少于Debian Linux的一半。
![](https://fuis.me/static/img/624fd3716db7eaa50aeef9c28b32d698.clipboard-2024-12-23.webp)

在图 5 中，很难区分最小化的 Linux 虚拟机和 Mirage。启动时间因 Xen 控制栈同步构建域而产生偏差，因为延迟通常不是虚拟机构建的首要关注点。我们修改了 Xen 工具栈以支持并行域构建，并在图 6 中分离出虚拟机启动时间。这清楚地区分了 Mirage 单内核和 Linux 之间的差异：Mirage 在 50 毫秒内启动。如此快速的重启时间减轻了通过重新配置进行重新部署过于繁重的担忧，同时也开辟了定期微重启的可能性[14]。
![](https://fuis.me/static/img/d765933b576e6346c851933e4fd63abd.clipboard-2024-12-23.webp)

## 线程

> 垃圾回收堆管理在单地址空间环境中更高效。通过消除多层调度可以减少线程延迟。

图 7a 对线程构建时间进行基准测试，展示了并行构建数百万个线程所需的时间，其中每个线程睡眠 0.5 到 1.5 秒然后终止。最接近传统云应用程序的 linuxpv 用户空间目标是最慢的，相同的二进制文件在原生 Linux 上运行时速度次之。由于测试受垃圾回收速度限制，使用不同内存分配器的两个 xen-目标表现明显更好：线程构建发生在 OCaml 堆上，因此创建数百万个线程会触发常规压缩和扫描。由于前面描述的专门地址空间布局（第 2 节），xen-运行时更快。使用大页面（xen-extent 与 xen-malloc 对比）几乎没有额外的好处，因为堆一次增长到最大大小，随后永远不会缩小。
我们还评估了线程计时器的精度：一个线程记录域系统时钟时间，休眠 1 到 4 秒，并记录系统时钟时间与预期唤醒时间之间的差值。图 7b 绘制了抖动的累积分布函数，并表明当唤醒数百万个并行线程时，单内核目标在延迟方面更低且更可预测。这仅仅是由于没有用户空间/内核边界，从而消除了 Linux 的系统调用开销。
![](https://fuis.me/static/img/cfd25ec98ee2090ca14d847082b110a7.clipboard-2024-12-23.webp)

## 网络和存储

> Unikernel 底层网络性能可与传统操作系统相媲美，尽管它是完全类型安全的。基于库的块管理性能与传统的分层存储栈相当。

作为针对 Linux 堆栈的简单延迟测试，我们从在其自己的虚拟机中运行的 Linux ping 客户端向两个目标发送大量的 $10^{6}$ ping：一个标准的 Linux 虚拟机，以及一个带有以太网、ARP、IPv4 和 ICMP 库的 Mirage 单内核。这对纯报头解析进行压力测试，而不引入 Linux 的任何用户空间元素。正如预期的那样，由于类型安全的轻微开销，Mirage 与 Linux 相比延迟略有增加（4-10%），但两者都在 72 小时的大量 ping 测试中存活下来。
我们使用 iperf 比较了 Mirage 的 TCPv4 协议栈（实现了完整的连接生命周期、快速重传和恢复、New Reno 拥塞控制以及窗口缩放）与 Linux 3.7 的 TCPv4 协议栈的性能。所有硬件卸载功能都被禁用，以便对 Mirage 进行最严格的测试：由于在 OCaml 中实现低级操作的开销天然高于在 C 语言中的开销，这意味着硬件卸载（特别是 TCP 分段）对 Mirage 性能的提升与对 Linux 的提升相比不成比例。性能与 Linux 相当：由于没有用户空间复制，Mirage 的接收吞吐量略高，而由于较高的 CPU 使用率，其传输性能较低。Linux 和 Mirage 都可以使千兆网络连接饱和，我们期望添加传输硬件卸载支持将使我们的实验性 TCP 协议栈甚至能够达到 10 Gb/s 的性能。
图 9 展示了使用 fio 对快速 PCI-express SSD 存储设备进行的简单随机读取吞吐量测试，比较了 Mirage xen-direct 设备与使用缓冲和直接 I/O 的 Linux RHEL5 内核（2.6.18）。同样，正如预期的那样，Linux 直接 I/O 和 Mirage 曲线实际上是相同的：两者都使用直接 I/O，因此对原始硬件性能的开销非常小。然而，Linux 缓冲缓存的性能影响是显著的：与避免缓冲缓存时实现的 1.6GB/s 相比，它使性能在大约 300MB/s 处趋于平稳。对于我们目标的设备来说，缺乏缓冲缓存并不重要：此类应用程序已经管理自己的存储。

## DNS 服务器应用

Mirage DNS 服务器设备包含核心库、网络栈中的以太网、ARP、IP、DHCP 和 UDP 库，以及一个以标准 Bind9 格式存储区域的简单内存文件系统。图 10 比较了 Xen 上的 Mirage 设备与两种同类最佳域名服务器的吞吐量：Bind 9.9.0，一个成熟且广泛使用的 DNS 服务器；以及 NSD 3.2.10，一个近期的高性能实现。
Bind 在合理的区域文件大小下实现了 55 kqueries/s 的速度。6 作为一个更近期专注于性能的重写版本，NSD 表现更好，达到约 70 kqueries/s。Mirage 设备最初表现非常差，但当我们引入对响应的记忆化以避免重复计算时，性能显著提高。一个简单的 20 行补丁，将性能从大约 40 kqueries/s 提高到 75–80 kqueries/s。误差线表示在 10 次运行中对 $\mu \pm \sigma$ 的无偏估计。
还有其他证据[26]表明，算法性能的提升大大超过仅由机器级优化带来的提升，而 Mirage 对函数式编程的运用为在这方面进行进一步实验提供了一个有效的平台。另一个例子是 DNS 标签压缩，它是出了名的，要正确处理很棘手，因为必须仔细跟踪标签片段。我们最初的实现使用了一个简单的可变哈希表，后来我们用一个函数式映射替换了它，使用了一个定制的排序函数，该函数在比较标签内容之前先测试标签的大小。这带来了大约 20%的速度提升，同时也防范了拒绝服务攻击，在这种攻击中，客户端故意造成哈希冲突。
DNS 也提供了一个很好的例子，说明 Mirage 的类型安全应该会带来显著的安全优势。例如，在过去的 10 年里，互联网系统联盟报告了 Bind 软件中的 40 个漏洞。其中，25%是由于内存管理错误，15%是由于对异常数据状态的处理不当，10%是由于错误的数据包解析代码，所有这些都可以通过 Mirage 的类型安全得到缓解。
最后，通过比较 Linux 和 Mirage 设备的大小可以看出 Mirage 的另一个主要优势：Mirage 设备大小为 183.5 kB，而 Linux VM 映像的使用大小为 462 MB。虽然这种差异的一部分可以归因于我们的库没有实现 BIND9 或 NSD 的完整功能集，但我们确实包含了 queryperf 测试套件所需的所有功能，并且 Mirage 设备足以在线自托管项目基础设施。
我们还测试了以 libOS 模式编译的 NSD 和 BIND，它们使用了 newlib-1.16 嵌入式 libc、lwIP-1.3.0 网络栈和标准的 Xen-4.1 MiniOS 设备驱动程序。性能显著低于预期，进一步的基准测试表明，这是由于 MiniOS 的 select(2)调度与 netfront 驱动程序之间的意外交互所致。我们在 C 语言 libOS 库方面的经验强化了我们的观念，即这种编程相当脆弱——使用嵌入式系统库通常意味着放弃性能（例如，优化的 libc 汇编被常见调用所取代）——而更有成效的方法是像 Drawbridge 对 Windows 7 所做的那样，将 Linux 内核分解为一个 libOS。

## OpenFlow 控制器

OpenFlow 是一种用于以太网交换机的软件定义网络标准[27]。它定义了一种架构和协议，通过该协议，控制器可以操作以太网交换机（称为数据路径）中的流表。Mirage 提供了实现 OpenFlow 协议解析器、控制器和交换机的库。通过链接到控制器库，设备可以直接控制硬件和软件 OpenFlow 交换机，但需遵守任何已有的网络管理策略。相反，通过链接到交换机库，设备可以像 OpenFlow 交换机一样被控制，这在设备提供网络层功能的场景中很有用，例如充当路由器、交换机、防火墙、代理或其他中间盒。作为软件实现，这些库可以根据特定设备需求进行扩展，例如，使流可以通过任一端点的 DNS 域进行标识，或者根据 HTTP URL 进行定义。
我们使用 OFlops 平台对我们的 OpenFlow 实现进行基准测试[28]。对于控制器基准测试，我们使用 cbench 模拟 16 个交换机同时连接到控制器，每个交换机服务 100 个不同的 MAC 地址。实验在具有 40GB RAM 的 16 核 AMD 服务器上运行，并且每个控制器配置为使用单个线程。我们以每秒处理的请求数来测量吞吐量，以响应由每个模拟交换机在两种场景下产生的数据包入消息流：批量模式，其中每个交换机维护一个完整的 64kB 输出数据包入消息缓冲区；以及单包模式，其中每个交换机只有一个数据包入消息在传输中。第一种场景测量处理请求时的绝对吞吐量，第二种场景测量控制器在公平地为连接的交换机提供服务时的吞吐量。
图 11 将 xen-direct Mirage 控制器与两个现有的 OpenFlow 控制器进行了比较：Maestro [29]，一个优化的基于 Java 的控制器；以及 NOX [30]的优化后的 C++ destiny-fast 分支，NOX 是最早且最成熟的公开可用 OpenFlow 控制器之一。不出所料，在两个实验中，优化后的 NOX 分支性能最高，尽管在批处理测试中确实表现出极端的短期不公平性。Maestro 更加公平，但性能显著下降，特别是在“单次”测试中，可能是由于 JVM 开销。Mirage 设备的性能介于 NOX 和 Maestro 之间，表明 Mirage 设法在保留高级语言特性（如类型安全）的同时，实现了优化后的 C++的大部分性能优势。

## 动态 Web 服务器应用

我们的最终设备实现了一个简单的“类似 Twitter”的服务。它维护一个内存中的推文数据库，并通过两个 API 调用进行测试：一个是获取个人的最后 100 条推文，另一个是向个人发布一条推文。Mirage 实现集成了第三方 Baardskeerder Btree 存储库，仅通过一个小补丁就移植到了 Mirage，以使用块 API 而不是 UNIX I/O。我们将 Mirage 单内核实现与运行 nginx、fastCGI 和 web.py 的基于 Linux 的设备进行比较。我们使用 httperf 基准测试工具作为同一物理机上的客户端，具有单独的专用内核，通过本地网桥连接以避免以太网成为瓶颈。每个 httperf 会话发出 10 个请求：1 条推文和 9 个“获取最后 100 条推文”。
图 12 展示了结果。单内核实现的可扩展性明显更好：在达到 CPU 限制之前，其扩展几乎是线性的，可扩展至大约 80 个会话（800 个请求——每个会话由 10 个 HTTP 请求组成，其中 9 个为 GET 请求，1 个为 POST 请求）。相比之下，Linux 虚拟机的性能要差得多，在大约 20 个会话时就达到了极限。作为对比，同一个 Linux 虚拟机通过 nginx 为两个客户端提供单个静态页面服务时，在达到 CPU 和 $f d$ 限制之前，可达到每秒 5000 个请求。尽管这一性能远高于（未优化的）Mirage 实现，但 Mirage 还有其他前面讨论过的优点：内存占用更小（32MB 对比 256MB）、类型安全和安全性。
图 13 比较了 Mirage 网络设备与使用 mpmworker 后端并根据 vCPU 数量调整工作进程数量以服务单个静态页面的标准 Linux VM（运行 Apache2）的性能。Linux VM 以三种配置运行：一个单独的 VM 分配 6 个 vCPU，两个 VM 各分配 3 个 vCPU，最后是 6 个 VM 各分配一个 vCPU。由于 Mirage 单内核不支持多核，因此只运行了一种配置，即 6 个单内核各分配一个 vCPU。结果首先表明，横向扩展似乎比拥有多个核心更能提高 Apache2 设备的性能；其次，在所有情况下，Mirage 单内核都超过了 Apache2 设备。

## 代码和二进制大小

由于广泛使用条件编译和复杂的构建系统等因素，直接比较代码行数（Lines-of-Code，LoC）很少有意义。我们试图通过根据合理的默认值进行配置来控制这些影响，然后进行预处理以去除未使用的宏、注释和空白。此外，为了尝试与预处理后 Linux 树中剩余的 700 万行代码进行公平比较，我们忽略了与我们没有类似物的组件相关的内核代码，例如 Linux 支持的许多架构、网络协议和文件系统。由于我们关注的是与底层虚拟机管理程序共享的面向网络的客户虚拟机，我们不包括 Xen 及其管理域的代码行数；这些可以根据需要单独进行分类[14,31]。
图 14a 展示了由 cloc 实用程序计算出的几个流行服务器组件的代码行数（Lines of Code，LoC）。即使去除不相关的代码，一个 Linux 设备所涉及的代码行数也至少是 Mirage 设备的 4 到 5 倍。请注意，尽管 Mirage 库的功能还不如行业标准的 C 应用程序丰富，但它们的库结构确保了在编译时可以去除未使用的依赖项，即使功能在不断增加。例如，如果不使用文件系统，那么整个块驱动程序集将自动被省略。在 Linux 发行版中对内核和用户空间进行这样的依赖关系分析并非易事。
编译后的二进制文件大小是对此的另一个有效说明，表 2 给出了基准设备的二进制文件大小。第一列使用了默认的 OCaml 死代码消除功能，它会删除未使用的模块，第二列使用了 ocamlclean（一种更广泛的自定义工具），它执行数据流分析以删除模块内未被引用的未使用函数；由于 Mirage 中没有动态链接，所以这是安全的[32]。无论哪种方式，所有的 Mirage 内核都比精简后的嵌入式 Linux 发行版紧凑得多，并且除了使用 Mirage API 构建应用程序之外，不需要程序员做任何工作。

# 讨论、相关工作

我们接下来讨论 unikernels 和 Mirage 与它们所基于的先前工作（来自类型安全、库操作系统和安全等领域）之间的关系。

## 类型安全和库操作系统

在操作系统中应用类型安全的愿望并不新鲜：已经有使用多种语言构建的类型安全操作系统，包括 Haskell[33,34]、Java[35]、标准 ML[21]、C#/.Net[4]和 Modula-3[36]。其中最后一个，SPIN[36]，在性质上可能与 Mirage 最为接近：SPIN 是一个可扩展的操作系统，它依赖于 Modula-3 的类型安全性来将扩展动态链接到内核中。最近，Singularity[4]对操作系统进行了重构，使其位于公共语言运行时之上，实现了许多类似的类型安全优势。单内核方法在某种程度上是正交的：它提议对操作系统进行重构，专门针对托管在公共云上的服务，这受益于但并不强制要求类型安全。这里展示的特定实现，即 Mirage，还使用了一种极其便携的函数式语言，这意味着 Mirage 核心和网络栈甚至可以编译为 JavaScript 以便在 node.js 上执行；并且对 ARM 和 FreeBSD 内核模块的移植在 alpha 状态下是可用的。
将操作系统重构为一组链接到应用程序中的库已经得到了广泛的探索。Exokernel[1]、Nemesis[2]和 Scout[37]都采用这种方法并取得了相当大的成功。实际上，用于高效网络 I/O 的页面重用特定技术（§3.5.1）直接受到 Exokernel 的高性能 Cheetah Web 服务器的启发[38]。最近，Drawbridge[3]将 Windows 改编为一个库操作系统，其中应用程序通过网络协议进行通信，这表明这种方法可以扩展到真正的商业操作系统。然而，以前的库操作系统由于硬件支持不佳而受到困扰，设备驱动程序要么需要从头编写，要么需要包装在兼容层中[39]。单内核系统使用类似的 libOS 技术，但将云作为部署平台，我们的 Mirage 实现使用 Xen 虚拟机管理程序作为低级通用硬件接口。
最后，Libra[5]使JVM直接位于Xen之上，专门针对Java工作负载。特别是，Libra使用在标准Linux虚拟机中运行的网关服务器来提供对标准操作系统服务（如网络栈和文件系统I/O）的访问。承载应用程序的虚拟机通过网关进程与其他虚拟机通信来访问这些服务。相比之下，Unikernels更加高度专业化且更完整，提供直接支持标准网络和文件系统协议的单用途设备虚拟机。

## 遗留系统支持

也许unikernel方法最明显的潜在问题是如何最好地支持遗留系统。我们采取的极端立场似乎确实需要大量重新实现。
如果使用我们的 Mirage 原型，可能会使用不熟悉的语言。例如，考虑 OpenSSL [40] 库中 SSL 协议的标准实现，或者像 exr2 这样仅在其标准实现的代码中有适当文档说明的存储格式：不能轻易地完全重写此类关键组件！另一方面，这些对安全至关重要的库目前呈现出复杂的 API，这常常导致严重的漏洞 [41]，而更高级的功能接口对于现有的 C 实现来说将是非常有价值的替代方案。
替代方法使用诸如 CIL 或 CCured（42、43）之类的工具为现有代码库改造类型安全。不幸的是，为特定的底层平台实现专门化技术被证明是相当大的工作量，并且很难将这些结果集成到单内核中。在范围的另一端，最近的工具如 HipHop [44] 朝着单内核方法迈出了一步，将 PHP 代码翻译成 C++，然后编译一个包含整个 PHP 应用程序的单一二进制文件。人们可以设想将这样的二进制文件进一步专门化为单内核，尽管使用 PHP 不会获得静态类型安全的好处。
我们采用了由 Flux OSKit [39]开创的方法，该方法封装了现有代码以将其移植到新系统中。Flux 通过针对多引导引导加载程序标准来实现这一点，然后包装来自诸如 Linux 等系统的设备驱动程序以使其适应其中，并且封装是一种非常有前途的技术，可应用于更大的云组件。例如，“大数据”处理系统，如 Map-Reduce、Hadoop 和 Dryad [45-47]通常被构造为一组相互通信的进程，分布在数据中心内。这些进程中的每一个都可以封装为单个虚拟机，并且虚拟机之间的消息传递可以通过 Vchan 实现。这种方法类似于 UNIX 权限分离[48]，并提供了一种增量部署路径，确保现有的可靠工程组件可以继续被使用，并且多个新的、未经测试的组件不必一次全部引入。

## 5.3 Deeper, Higher, Broader

Mirage 单内核实现的一个缺点是它仍然依赖大量不安全的代码，特别是 Xen 虚拟机管理程序和管理域 dom0。这就提出了一个问题，我们能否进一步推动单内核向下发展？这将需要要么重新实现虚拟机管理程序本身，要么对于资源更受限的平台，完全放弃它，转而专注于单个封装的应用程序。在这两种情况下，当前的许多方法（例如 I/O 堆栈）都将得以延续，但对多核硬件的原生支持将需要额外的工作。
有趣的可能性是在 Barrelfish [15] 软件的基础上进行构建。与在单个操作系统下运行的进程相比，现有的云编排层，如 OpenStack 或 Eucalyptus [49]，在操作小型虚拟机时会表现出高延迟。单内核系统依赖于运行多个虚拟机来实现并行化，因此在这个领域需要进行改进。最后，使用协同线程确实需要某种形式的更广泛的管理系统，因为一个错误可能会使整个设备完全死锁。
与此同时，问题出现了，抽象的程度可以提高到多高。由于全面且相当准确的 RFC 规范，互联网协议相对容易实现。然而，更高级别的服务（例如文件系统、加密库和中间件）通常最多只是非正式地指定。幸运的是，社区正在开发用于指定正确实现的工具[50,51]，并且一些定理证明器可以从这些规范中提取 OCaml 代码[52]。探索将验证技术应用于关键系统组件（特别是编译器后端和操作系统微内核）的工作也很有前景[53,54]。
最后，从更广泛的角度思考，为云进行开发和部署引发了有关工作流和编排的问题。现实世界中的云计算部署已经拥有日益复杂的编排系统来管理虚拟机的远程部署，并且我们设计 Mirage 以利用这一点（例如，具有较小的部署二进制大小）。图 15 说明了 Mirage 代码是如何在类 UNIX 系统上进行本地开发和测试，然后编译为 unikerel 以部署到云中的。开发人员首先构建 posix 套接字目标（target）在字节码解释器中起连接作用，利用主机内核的网络堆栈，并将键值存储中的键映射到本地文件。这提供了一个熟悉的开发环境，可用于调试和分析应用程序逻辑。接下来，开发人员应用第一个特殊化步骤，针对 posix-direct 目标进行构建。这消除了对标准操作系统套接字实现的依赖，而是连接到单内核网络堆栈（§3.5），并生成一个本机代码二进制文件，该文件使用 tuntap 处理以太网流量，并将配置文件直接编译到二进制文件中。结果是 I/O 处理在应用程序内部执行，尽管由于 tuntap 引起的数据复制而效率低下。这允许使用 UNIX 开发工具在本地测试单内核，而不是在调试微内核时使用稀疏的工具，有助于隔离应用程序逻辑、Mirage 库中的错误，或由于两者之间的交互而产生的错误。
最后，开发人员通过 xen-direct 目标进一步进行专门化，生成独立的单内核。应用程序代码经过交叉编译，并与安全的 Mirage 设备驱动程序和死代码消除功能链接在一起：如果应用程序不使用某个组件（例如 TCP），则该组件不会被编译进去。结果是一个可在本地 Xen 或通过公共云服务启动的虚拟机映像。该映像比等效的基于 Linux 的发行版小得多（图 14），并且至关重要的是，所有 I/O 流量都由类型安全代码处理，具有专门化运行时的性能和安全优势。

# 结论

我们提出了单内核方法，以显著提高为云构建和部署设备的安全性和效率。基于早期在库操作系统方面的工作，我们贡献了一个用于库操作系统的静态类型安全的 OCaml 原型的设计和评估，包括一套完整的全新协议库，确保部署的单内核从一开始就是内存安全的。我们的方法还可以选择通过对这种专用虚拟机的特殊支持来扩展管理程序，以提高运行时安全性和启动延迟。
通过我们的实验实施，我们展示了通过转向新颖的编程风格（如 OCaml 所提供的那些风格）来放宽源代码级别的向后兼容性要求，如何实现安全性和效率方面的益处。我们的评估表明，在所有情况下，这些益处对性能的影响微乎其微甚至没有影响，并且在某些情况下实际上还可以提高性能。总的来说，我们已经证明了商品云计算平台与现代语言运行时的结合是富有成效的。单内核系统可以作为一系列未来研究的平台，探索如何更好地利用丰富的云计算资源，特别是对于那些受益于跨核心和主机水平扩展的分布式应用程序。Mirage 原型的代码和我们的实验脚本是开源的，可在 http://openmirage.org 上以 BSD 风格的许可证下载。
