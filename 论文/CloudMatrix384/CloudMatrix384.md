# CloudMatrix384

[华为云发布 CloudMatrix 384 超节点 多项性能全面突破](https://www.huaweicloud.com/intl/zh-cn/news/20250424094932570.html)

> 2025 年 4 月 10 日，以“聚力共创，加速行业智能跃迁”为主题的华为云生态大会 2025 在安徽芜湖召开。会上，华为公司常务董事、华为云计算 CEO 张平安公布了 AI 基础设施架构突破性进展，推出 CloudMatrix 384 超节点，并宣布已在芜湖数据中心规模上线。
> 如何让 AI 更好地从实验室走向产业落地，成为时代发展的“必答题”。而 CloudMatrix 384 超节点，就是华为云给出的答案。

![[Pasted image 20250721144209.png|500]]

背景：TBD，NVIDIA 禁运，需要对比 [NVIDIA GB200 NVL72](https://www.nvidia.com/zh-tw/data-center/gb200-nvl72/)

# 摘要

LLMs 的快速发展带来了：

- 参数规模的不断扩大
- 专家混合（MoE）架构的采用
- 上下文长度的增加
  对人工智能基础设施提出了前所未有的要求。
  传统的人工智能集群受到
- [[计算强度]]
- 内存带宽限制
- 芯片间通信开销
- 严格延迟要求的制约
  在实际部署中，为了满足严格的服务等级目标，还需要处理多样化、突发性的工作负载、可变长度的输入以及不均衡的专家激活，这些都进一步加剧了上述挑战。克服这些制约因素需要一个从根本上重新架构、协同设计的软硬件栈。

华为 CloudMatrix384 是这一愿景的首个生产级实现，它将：

- 384 个昇腾 910 NPU
- 192 个鲲鹏 CPU
- 及其他硬件组件
  集成到一个统一的超级节点中，通过超高带宽、超低延迟的统一总线（UB）网络进行互连。
  与传统的分层设计不同，该架构通过 UB 实现直接的全互联通信，允许计算、内存和网络资源进行动态池化、统一访问和独立扩展。这些架构特性对于大规模 MoE 专家并行和分布式键值（KV）缓存访问等通信密集型操作尤其有利，使 CloudMatrix384 成为可扩展、高性能的新一代 LLM 服务基础。

为了充分利用 CloudMatrix384 的能力，我们提出了 CloudMatrix-Infer，这是一个全面的 LLM 服务解决方案，为部署 DeepSeek-R1 等大规模 MoE 模型建立了最佳实践。
CloudMatrix-Infer 包含三项核心创新。

- 点对点服务架构，将预填充、解码和缓存分解为可独立扩展的资源池。
  与现有以 KV 缓存为中心的架构不同，这种设计通过 UB 网络实现了对缓存数据的高带宽、统一访问，从而减少了数据局部性限制，简化了任务调度，并提高了缓存效率。
- 大规模专家并行（EP）策略，利用 UB 网络实现高效的令牌分派和专家输出组合
  该策略支持非常大的 EP 度，例如 EP320，使得每个 NPU 芯片恰好承载一个专家，从而实现低解码延迟。
- 硬件感知优化，提高了执行效率和资源利用率
  包括高度优化的算子、基于微批次的流水线以及 INT8 量化
  评估。CloudMatrix-Infer 在不牺牲准确性的前提下，实现了最先进的效率：
- DeepSeek-R1 模型
- CloudMatrix-Infer 的预填充（prefill）吞吐量达到每 NPU 6,688 tokens/s
- 解码（decode）吞吐量达到每 NPU 1,943 tokens/s（在<50 毫秒 TPOT 下）
  这些结果对应的计算效率为：
- 预填充 4.45 tokens/s/TFLOPS
- 解码 1.29 tokens/s/TFLOPS
  均超过了 SGLang 在 NVIDIA H100 和 DeepSeek 在 NVIDIA H800 上公布的结果
  CloudMatrix-Infer 还有效地管理了吞吐量-延迟权衡，即使在更严格的低于 15 毫秒 TPOT 约束下，也能保持每 NPU 538 tokens/s 的解码吞吐量。此外，Ascend 910 上的 INT8 量化在 16 个不同的基准测试中，保持了与 DeepSeek-R1 官方 API 相当的模型准确性。

# 引言

LLM 发展趋势：

- 现代 LLMs，如 DeepSeek-R1 (DeepSeek-AI et al, 2025)、LLaMA-4 (Meta AI, 2025)和 Qwen-3 (Qwen Team, 2025)，其规模通常达到数千亿甚至数万亿参数
- MoE 模型通过为每个 token 选择性地激活一小部分专家来引入结构稀疏性，从而在规模上实现更高的效率，同时在专家路由和同步方面引入了新的系统级挑战
- 上下文窗口已从数万个 token 扩展到超过一百万个 token - 给注意力计算和键值（KV）缓存存储带来了巨大压力 - KV 缓存总容量随并发用户数量线性增长，这给 KV 缓存在系统中的分布、放置和访问带来了巨大限制
  这些趋势共同给 AI 基础设施带来了巨大压力，需要海量计算能力、高内存容量和带宽、密集的芯片间通信以及严格的延迟限制，最终将传统 AI 集群推向其可扩展性极限。

实际服务更复杂。

- LLM 服务系统必须适应变长用户输入
- Token 间不均衡的专家激活

为响应这些需求，我们提出了华为 CloudMatrix，这是一种基于全对等高带宽互联和细粒度资源解耦原则构建的新一代 AI 数据中心架构。我们特别重点介绍 CloudMatrix384，它是这一创新架构理念的首个生产级实现。CloudMatrix384 是一个专为大规模 AI 工作负载打造的 AI 超级节点，采用全对等互联硬件设计。
CloudMatrix384 由 384 个昇腾 910 NPU 和 192 个鲲鹏 CPU 组成，通过名为统一总线（UB）的超高带宽、超低时延网络互联。特别是，该 UB 网络支持所有计算和内存组件之间的直接全对全数据交换。与节点内和节点间互联带宽不均衡的传统分层架构不同，CloudMatrix384 使整个超级节点能够作为一个逻辑上统一、紧密耦合的计算实体运行，体现了“万物可池化、平等对待、自由组合”的全对等原则。这些架构特性对于大规模 MoE 专家并行和分布式 KV 缓存访问等通信密集型操作尤其有利，使 CloudMatrix384 成为下一代 LLM 服务的可扩展、高性能基础。

CloudMatrix384 的最初设计早于 MoE 架构的广泛采用，因为这样一个全面的超级节点系统的设计和部署通常需要数年时间。尽管如此，CloudMatrix384 的构建初衷就是为了增强互连带宽和通信效率——这些核心能力对于扩展大规模训练和推理工作负载至关重要。像 DeepSeek-R1 这样的大规模 MoE 模型的出现，验证了这种架构上的前瞻性，凸显了在现代 LLM 部署中，通信带宽与计算和内存带宽能力同等重要。

为充分发挥 CloudMatrix384 的能力，我们提出了 CloudMatrix-Infer，这是一个全面的 LLM 服务解决方案，代表了部署诸如 DeepSeek-R1 等大规模 MoE 模型的最佳实践。CloudMatrix-Infer 引入了三项核心创新。

首先，我们设计了一种新颖的对等服务架构，该架构将 LLM 推理系统分解为三个独立的子系统：预填充、解码和缓存。
对等意味着这三个子系统作为对等且独立的资源池运行，而不是围绕一个中心化实体进行编排。这与传统的以 KV 缓存为中心的架构 (Qin et al, 2025; NVIDIA Corporation, 2025) 形成鲜明对比，后者将请求调度与缓存 KV 块的物理位置紧密耦合，增加了调度复杂性并限制了资源分配的灵活性。通过利用高带宽 UB 互连，我们构建了一个解耦的内存池，为整个系统提供共享缓存服务。预填充和解码子系统中的所有 NPU 都可以以对等的方式直接从此池中访问缓存的 KV 数据，并享有统一的带宽和延迟，而无需考虑数据最初在何处计算或存储。该设计将请求调度与数据局部性解耦，极大地简化了任务调度逻辑，提高了缓存效率，并增强了整个系统的资源利用率。

第二，我们开发了一种专为 MoE 模型优化的大规模专家并行 (LEP) 策略。LEP 的核心原理是聚合大量 NPU 的计算能力和内存带宽，以加速注意力机制和前馈网络的计算。这种加速是以增加通信开销为代价的，而通信开销的增加源于令牌分发和专家输出的组合。
然而，CloudMatrix384 的超高带宽 UB 互连确保了这种通信延迟保持在有限范围内，并且不会成为主要的性能瓶颈。此外，我们的 LEP 策略支持极高程度的专家并行（例如 EP320），使得**每个 NPU die 能够为 DeepSeek-R1 精确地托管一个专家**。这种配置最大限度地减少了同一 rank 内专家之间的串行执行，从而降低了 MoE 的整体执行延迟。总之，这些设计选择共同实现了低解码延迟，并为基于 MoE 的推理带来了显著的端到端性能提升。

最后，我们介绍了一套专为 CloudMatrix384 量身定制的硬件感知优化方案，包括高度优化的昇腾算子、基于微批次的流水线以及 INT8 量化。经过优化的算子能够加速端到端执行，并为 LEP 提供高效支持。基于微批次的流水线设计通过重叠处理两个连续的微批次，提升了资源利用率和系统吞吐量。INT8 量化提升了计算效率，并显著降低了内存带宽消耗。总的来说，这些优化与 CloudMatrix384 独特的架构特性协同设计，包括片上立方、向量和通信引擎，以及高带宽 UB 互连，旨在最大化整体执行效率。

我们在 CloudMatrix384 上使用 6710 亿参数的 DeepSeek-R1 模型对 CloudMatrix-Infer 进行了评估，结果显示其具有卓越的性能和硬件效率。
在预填充阶段，对于 4K 提示长度，CloudMatrix-Infer 实现了每个 NPU 6,688 tokens/s 的吞吐量。这相当于每 TFLOPS 4.45 tokens/s 的计算效率。
在解码阶段，对于 4K KV 缓存长度，该系统可维持每个 NPU 1,943 tokens/s 的吞吐量，同时将每个输出令牌的时间（TPOT）稳定保持在 50 毫秒以下，效率达到每 TFLOPS 1.29 tokens/s。
值得注意的是，这两个阶段的计算效率指标均超过了在 NVIDIA H100 上运行的 SGLang 和在 NVIDIA H800 上运行的 DeepSeek 等领先框架。
CloudMatrix-Infer 还展示了对吞吐量与延迟这一基本权衡的有效管理。为满足低于 15 毫秒的更严格 TPOT 要求，CloudMatrix-Infer 能够动态调整其批处理大小，实现每个 NPU 538 tokens/s 的解码吞吐量。这凸显了其在不同服务水平目标下的可预测性能和适应能力。此外，在 16 个代表性基准测试中，INT8 量化保持了与官方 DeepSeek-R1 API 相当的精度。这些结果共同证明了，CloudMatrix384 结合我们的点对点服务解决方案 CloudMatrix-Infer，是用于大规模 LLM 部署的一个可扩展、高吞吐的生产级平台。

# LLM 趋势

LLMs 的快速发展呈现出三个显著趋势：

- 模型参数数量的不断增加
- 通过混合专家（MoE）架构引入稀疏性
- 以及上下文窗口的扩展
  这些发展旨在提升模型性能，同时解决计算效率和可扩展性问题。

Scaling law 表明，增加 LLMs 的参数数量会带来模型在各种任务上性能的提升。

- Meta 的 Llama 4 Behemoth 模型拥有近 2 万亿参数，而其对应的 Llama 4 Maverick 模型则包含 4000 亿参数
- 由 DeepSeek-AI 开发的 DeepSeek-V3 包含 6710 亿参数
- 谷歌的 PaLM 模型包含 5400 亿参数
- xAI 的 Grok-1 模型则具有 3140 亿参数

为应对日益增长的训练和推理成本，现代 LLMs 越来越多地采用稀疏激活的 MoE 架构，这种架构将模型总容量与每个 token 的计算需求解耦。

- 著名的实现包括 Mixtral 8×7B，该模型总参数量为 467 亿，但通过将每个 token 路由到每层 8 个专家中的 2 个，每个 token 仅激活 129 亿参数，从而在保持计算效率的同时，实现了与 GPT-3.5 相媲美的性能 (Jiang et al, 2024)
- Databricks 的 DBRX 采用了一种细粒度的 MoE 架构，总参数量为 1320 亿，通过从 16 个较小的专家中选择 4 个，每个 token 激活 360 亿参数，从而提高了吞吐量并降低了延迟 (Frankle et al, 2024)
- Meta 的 Llama 4 系列在开源模型中引入了 MoE，其中 Llama 4 Maverick 使用 128 个专家，Llama 4 Scout 使用 16 个专家，两者每个 token 都保持 170 亿的激活参数量 (Meta AI, 2025)
- DeepSeek-V3 在其前代模型的基础上进行了扩展，将每层的路由专家数量从 160 个增加到 256 个，从而在不按比例增加计算负载的情况下提升了模型容量 (DeepSeek-AI et al, 2024a, b)
- 阿里巴巴的 Qwen3-235B 模型集成了 128 个专家，每个 token 激活 220 亿参数，在兼顾大规模容量的同时实现了计算效率 (Qwen Team, 2025)
- 华为的盘古 Ultra MoE 模型扩展至 7180 亿参数，每个 token 的激活参数为 390 亿。该模型采用 MoE 架构，每层包含 256 个专家，每个 token 激活其中 8 个 (Tang et al, 2025)。

LLMs 中上下文窗口的扩展使其能够处理更长的序列，这对于需要扩展推理和连贯性的任务至关重要。近期的进展反映了这一转变：

- OpenAI 的 GPT-4.5 支持 128,000 个词元（token）的上下文窗口 (OpenAI, 2025)
- 而谷歌的 Gemini 2.5 Pro 则提供高达 100 万个词元（token）的上下文窗口 (Google DeepMind, 2025)。
  诸如 LongBench (Bai et al, 2024) 等基准测试量化了扩展上下文窗口在问答、摘要和多步推理等任务中的优势。
  然而，向 LLMs 输入长提示会显著增加计算成本并延长推理延迟。为降低这些成本，生产系统采用上下文缓存技术，即将在先前提示片段中生成的键值 (KV) 块存储起来，并在后续的对话轮次或请求中重复使用。这种方法消除了对提示的冗余注意力计算，从而降低了延迟并提高了效率

在此背景下，我们识别出四个关键的系统级挑战：

1. 扩展通信密集型并行。随着模型规模的增长，最先进的人工智能模型通常会超出单个计算节点的容量，因此需要多节点并行策略。虽然现有的 AI 集群通过 RDMA 网络支持节点间通信，但其带宽和拓扑通常是为数据并行或流水线并行（DP/PP）优化的，这些并行策略涉及的节点间流量不大。然而，张量并行（TP）和专家并行（EP）需要频繁、细粒度和低延迟的通信，这种通信很难跨节点边界进行有效扩展。这迫使许多部署将 TP/EP 组限制在单个计算节点内，从而限制了可扩展性。
2. 在异构 AI 工作负载下保持高利用率。现代 AI 工作负载表现出高度多样化和动态的资源需求。训练通常是计算密集型的，推理（特别是 LLMs 的解码阶段）通常受内存带宽限制，而自动驾驶模型训练等任务则涉及大量的 CPU 端数据预处理。固定的节点配置无法高效地适应这种多样性，常常导致过度配置或利用率不足。为了最大化效率和适应性，现代 AI 基础设施必须能够根据每个工作负载的特定需求，对 NPU、CPU 和内存等异构资源进行动态、细粒度的组合。
3. 实现 AI 与数据密集型工作负载的融合执行。AI 工作流日益与数据引入、预处理、检索、分析和模拟等传统数据密集型操作交织在一起。与此同时，数据库、大数据和高性能计算（HPC）等通用工作负载自身也在不断演进，以融合 AI 功能。这些融合执行模式要求高吞吐、低延迟的通信和灵活的资源编排。然而，主要为常规通用工作负载优化的传统数据中心基础设施难以满足这些严苛的要求。要实现 AI 与数据密集型任务的高效融合，需要一种全新的基础设施。
4. 提供内存级存储性能。现代人工智能流水线在处理前所未有的数据规模时，其要求远超传统存储系统的能力。诸如摄取 PB 级数据集、管理 TB 级模型检查点以及支持延迟敏感型推理（尤其是在使用大型 KV 缓存和检索增强生成 (RAG) 模块时）等任务，都需要存储子系统具备内存级的带宽、延迟和 IOPS。围绕基于磁盘的访问模式设计的传统存储层次结构，常常成为性能瓶颈，因数据匮乏而导致 NPU 利用率不足。

# 愿景

![[Pasted image 20250721160558.png]]
图 1. 华为 CloudMatrix 架构愿景彻底重塑了 AI 数据中心基础设施。它打破了传统的孤岛式设计，通过统一的超高性能网络，实现了 CPU、NPU、内存、网卡及其他资源的完全对等解耦和池化，为可扩展的 AI 原生数据中心奠定了基础。
该架构围绕全对等高带宽互连与细粒度资源解耦的原则精心设计。如图 1 的概念性概述所示，CloudMatrix 超越了传统的以 CPU 为中心的层级式设计。它实现了所有异构系统组件（包括 NPU、CPU、DRAM、SSD、网卡（NIC）和领域专用加速器）之间的直接、高性能通信，尤其无需 CPU 居中协调。

该架构的核心是超高带宽、低延迟的统一总线（UB）网络，可实现高效的全系统数据流动与协同。CloudMatrix 基于这一互联基底，提供了四大基础能力，共同定义了 AI 原生基础设施的新范式：

1. 面向 TP/EP 的可扩展通信。UB 互连支持 NPU 之间的直接、高吞吐量对等通信，使 TP 和 EP 组能够扩展到单个节点之外。这消除了节点间的瓶颈，并允许大模型高效地分布在整个超级节点上。
2. 针对异构工作负载的灵活资源组合。CloudMatrix 将 CPU、NPU 和内存解耦为独立的资源池，从而实现由工作负载驱动的细粒度组合。这种灵活性允许根据工作负载的需求进行细粒度的资源分配，例如，内存密集型缓存节点、CPU 密集型预处理节点，从而使部署摆脱了固定节点配置或基于 PCIe 的主机-设备耦合的限制。
3. 融合工作负载的统一基础设施。高带宽 UB 网络在单一的纵向扩展 (scale-up) 基础设施内，同时支持 AI 和数据密集型应用。这实现了 LLM 推理、训练、模拟和分析等工作负载的融合执行，满足了混合式 AI 流水线日益增长的普遍需求。
4. 通过解耦内存池实现内存级存储。CloudMatrix 将集群中与 CPU 连接的 DRAM 聚合为一个可通过 UB 访问的共享高性能内存池。该底层基础架构为弹性内存服务（EMS）(Huawei Cloud, 2025a) 等服务提供支持，通过消除传统 I/O 瓶颈，加速了 KV 缓存重用、参数加载和模型检查点等对延迟敏感的操作。
