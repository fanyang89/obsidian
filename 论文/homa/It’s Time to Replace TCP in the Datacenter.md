[原文：It’s Time to Replace TCP in the Datacenter](https://arxiv.org/pdf/2210.00714)

# 摘要

尽管 TCP 有着悠久而成功的历史，但对于现代数据中心来说，它是一种糟糕的传输协议。TCP 的每一个重要元素，从面向流到期望按顺序发送数据包，对数据中心来说都是错误的。现在是时候认识到 TCP 的问题过于基本，相互关联以至于无法修复；利用现代网络全部性能潜力的唯一方法是在数据中心引入一种新的传输协议。Homa 证明了创建一种避免 TCP 所有问题的传输协议是可能的。尽管 Homa 与 TCP API 不兼容，但通过将其与 RPC 框架集成，应该有可能使其广泛使用。

# 导论

TCP 传输协议已被证明是非常成功和适应性强的。在 20 世纪 70 年代末设计 TCP 时，只有大约 100 台主机连接到现有的 ARPANET，网络链路的速度达到数十千比特/秒。从那以后的几十年里，互联网已经发展到数十亿台主机，100 千兆比特/秒或更高的链路速度是司空见惯的，然而 TCP 仍然是几乎所有应用程序的主力传输协议。设计出一种能够在底层技术如此剧烈的变化中幸存下来的机制是一项非凡的工程成就。
然而，数据中心计算为 TCP 带来了前所未有的挑战。数据中心环境拥有数以百万计的核心，单个应用程序利用数千台在微秒时间尺度上进行交互的机器，TCP 的设计者无法想象这种环境，TCP 在这种环境中表现不佳。TCP 仍然是大多数数据中心应用程序的首选协议，但它在许多层面上引入了开销，从而限制了应用程序级的性能。例如，众所周知，TCP 在混合工作负载下遭受短消息的高尾部延迟。TCP 是“数据中心税”的主要贡献者，这是一组低级开销，消耗了数据中心所有处理器周期的很大一部分。
这份立场文件认为，TCP 在数据中心面临的挑战是不可逾越的。第 3 节讨论了 TCP 中的每一个主要设计决策，并证明每一个决策对数据中心来说都是错误的，会产生重大的负面后果。其中一些问题在过去已经讨论过，但将它们放在一个地方是有启发性的。TCP 的问题影响多个层面的系统，包括网络、内核软件和应用程序。一个例子是负载平衡，它在数据中心中至关重要，以便同时处理高负载。负载平衡在 TCP 设计时并不存在，TCP 会干扰网络和软件中的负载平衡。
第 4 节认为 TCP 不能以进化的方式修复；有太多的问题和太多的联锁设计决策。相反，我们必须找到一种方法将完全不同的传输协议引入数据中心。第 5 节讨论了数据中心的良好传输协议应该是什么样子，以 Homa 为例。Homa 是以全新的方式设计的，以满足数据中心计算的需求，实际上它的每一个主要设计决策都与 TCP 不同。因此，一些问题，如网络核心结构中的拥塞，被完全消除了。其他问题，如拥塞控制和负载平衡，变得更容易解决。Homa 证明了解决 TCP 的所有问题是可能的。
由于 TCP 根深蒂固的地位，它不太可能很快被完全取代，但是 TCP 可以通过将 Homa 集成到少数现有的 RPC 框架（如 gRPC）中来取代许多应用程序。使用这种方法，Homa 不兼容的 API 将仅对框架开发人员可见，应用程序应该能够相对容易地切换到 Homa。

# 需求

- 可靠的传输。该协议必须在网络出现瞬时故障的情况下，可靠地将数据从一台主机传输到另一台主机。
- 低延迟。现代网络硬件使短消息的往返时间只有几微秒。传输协议不能显著增加这种延迟，这样应用程序就会经历接近硬件限制的延迟。传输协议还必须支持尾部的低延迟，即使在相对较高的网络负载和混合流量的情况下也是如此。尾部延迟对传输协议来说尤其具有挑战性；尽管如此，应该有可能在最佳情况延迟的 2-3 倍内实现短消息的尾部延迟。
- 高吞吐量。传输协议必须以两种不同的方式支持高吞吐量。传统上，术语“吞吐量”指的是数据吞吐量：在单个消息或流中传递大量数据。这种吞吐量仍然很重要。此外，数据中心应用程序需要高消息吞吐量：能够快速发送大量小消息，用于广播和 shuffle 等通信模式。消息吞吐量在历史上没有受到太多关注，但在数据中心中至关重要。
  为了满足上述需求，还需要处理以下问题：
- 拥塞控制。为了提供低延迟，传输协议必须限制网络队列中数据包的堆积。数据包排队可能发生在边缘（连接主机到机架顶部交换机的链路）和网络核心；每种形式的拥塞都会产生不同的问题。
- 跨服务器内核的高效负载平衡。十多年来，网络速度一直在快速增长，而处理器时钟速率几乎保持不变。因此，单个内核不再可能跟上单个网络链路；传入和传出负载都必须分布在多个内核上。在多个级别都是如此。在应用程序级别，高吞吐量服务必须在多个内核上运行，并在内核之间分配它们的工作。在传输层，单个内核无法跟上高速链路，尤其是短消息。负载平衡以两种方式影响传输协议。首先，它会引入开销（例如，使用多个内核会导致额外的缓存未命中以实现一致性）。其次，负载平衡会导致热点，其中负载在内核之间分布不均匀；这是软件级别的拥塞形式。负载平衡开销现在是尾部延迟的主要来源之一，它们受到传输协议设计的影响。
- 网卡卸载。越来越多的证据表明，基于软件的传输协议不再有意义；它们根本无法以可接受的成本提供高性能。例如： - 最好的软件协议实现的端到端延迟是应用程序通过内核旁路直接与网卡通信的实现的 3 倍以上。 - 与 NIC 卸载实现相比，软件实现在小消息吞吐量方面放弃了 5-10 倍。 - 在双向以 80% 的利用率驱动 100Gbps 网络仅在网络堆栈中就消耗 10-20 个内核。这不是一种经济高效的资源使用方式。
  因此，在未来，传输协议将需要转移到专用网卡硬件。传输协议不得具有排除硬件实现的功能。请注意，基于网卡的传输不会消除软件负载平衡作为一个问题：即使传输在硬件中，应用软件仍然会分布在多个内核中。

# TCP 的一切都是错的

本节讨论 TCP 的五个关键属性，几乎涵盖了其所有设计：

- 面向流
- 面向连接
- 共享带宽 (“公平” 调度)
- 发送端驱动的拥塞控制
- 按序数据包交付
  这些属性中的每一个都代表了数据中心传输的错误决策，并且这些决策中的每一个都具有严重的负面后果。

## 面向流

TCP 的数据模型是一个字节流。然而，这不是大多数数据中心应用程序的正确数据模型。数据中心应用程序通常交换离散的消息来实现远程过程调用。当消息在 TCP 流中序列化时，TCP 不知道消息边界。这意味着当应用程序从流中读取时，不能保证它会收到完整的消息；它可能接收不到一条完整的消息，或者几条消息的一部分。基于 TCP 的应用程序在序列化消息时必须标记消息边界（例如，通过在每条消息前面加上其长度），并且它们必须使用这些信息在收到消息时重新组装消息。这会带来额外的复杂性和开销，例如为部分接收的消息维护状态。
流模型对于软件负载平衡来说是灾难性的。考虑一个应用程序，它使用一组线程来处理通过一组流到达的请求。理想情况下，所有线程都将等待任何流上的传入消息，消息分布在线程之间。但是，对于字节流模型，不能保证读取操作会返回整个消息。如果多个线程都从一个流中读取，则单个消息的一部分可能会被不同的线程接收。原则上，线程可以在其中一个线程中协调和重新组装整个消息，但这过于昂贵而不实用。
相反，TCP 应用程序必须使用两种较差的负载均衡形式之一，即每个流由单个线程拥有。第一种方法，如 memcached 所使用的，是在线程之间静态划分一组流，每个线程处理其流上到达的所有请求。这种方法容易产生热点，即一个线程接收到不成比例的大量传入请求。第二种方法，如在 RAMCloud 中使用的，专门安排一个线程读取来自所有流的所有传入消息，然后将消息分派给其他线程进行服务。这允许在工作线程之间实现更好的负载均衡，但分派线程会成为吞吐量瓶颈。此外，每个请求需要经过两个单独的线程，这增加了大量的软件开销并增加了延迟。因此，只有在请求服务时间相对较长时，分派线程方法才有效。
流的根本问题是接收数据的单位（字节范围）与可分派的工作单位（消息）不对应。唤醒线程以接收部分消息是没有意义的；它将无法处理消息，直到它接收到整个消息。此外，如果一个线程在一次读取操作中接收到多条消息，它一次只能处理其中一条；最好将每条消息分派到不同的线程，这样消息就可以同时处理。
流式传输对负载平衡的负面影响将延续到未来的世界，在这个世界中，传输处理被卸载到网卡。在这个世界中，网卡应该执行负载平衡，通过内核绕过跨应用程序线程集合分派传入请求。然而，这是不可能的，因为关于消息边界的信息对传输层来说是应用程序特定，未知的。应用程序仍然必须使用上述方法之一，每种方法都会影响延迟和/或吞吐量。
流式传输对尾部延迟有额外的影响，因为它会导致行头阻塞。在单个流中发送的消息必须按顺序接收；这意味着短消息可能会在同一流中的长消息之后延迟。我们在 RAMCloud 中观察到这种现象，从一台服务器到另一台服务器的小型时间敏感复制请求可能会被日志压缩的长后台请求延迟，导致写入延迟增加 50 倍。
最后，流提供的可靠性保证并不适合应用程序。应用程序需要往返保证。客户端应用程序需要保证其请求将被传递和处理，并且它将收到响应；如果其中任何一个失败，客户端希望收到错误通知。但是，流只保证在一个方向上传递数据。如果服务器不发送响应，客户端将不会收到任何通知，在某些情况下，如果服务器机器崩溃，也不会收到任何通知。因此，客户端必须实现自己的端到端超时机制，即使 TCP 已经有自己的计时器。这些机制引入了额外的开销。

## 面向连接

TCP 要求应用程序与之通信的每个对等点都有长期存在的连接状态。在数据中心环境中，连接是不受欢迎的，因为应用程序可能有成百上千个连接，导致空间和/或时间上的高开销。例如，Linux 内核为每个 TCP 套接字保留大约 2000 字节的状态，不包括数据包缓冲区；应用程序级别需要额外的状态。
Facebook 发现，在每个应用程序线程和每个服务器之间建立单独连接所需的内存“极其昂贵”。为了减少这些开销，应用程序线程通过一组代理线程进行通信，这些代理线程管理与所有服务器的连接。这允许每个服务器的单个连接在该主机上的所有应用程序线程之间共享，但它增加了通过代理进行通信的开销。为了减少代理开销，Facebook 使用 UDP 而不是 TCP 来处理可以容忍 UDP 不可靠性的请求，但这牺牲了拥塞控制。
由于 NIC 芯片上的资源有限，在将传输卸载到 NIC 时，连接状态的开销也很成问题。这个问题在 Infiniband 社区中是众所周知的。多年来，RDMA NIC 只能缓存几百个连接的状态；如果活动连接的数量超过缓存大小，信息必须在主机内存和 NIC 之间打乱，性能损失相当大。
连接的另一个问题是，在传输任何数据之前，它们需要一个设置阶段。在 TCP 中，设置阶段有一个不小的成本，因为它需要主机之间的额外往返。传统上，连接的寿命很长，因此设置成本可以在大量请求中摊销。然而，在新的无服务器环境中，应用程序的生命周期非常短，因此很难摊销连接设置的成本。
网络社区似乎有一个信条，即为了实现可靠交付和拥塞控制等理想属性，需要连接，但连接成本很高，第 5 节将表明，没有连接也可以实现这些属性。

## 带宽共享

在 TCP（传输控制协议）中，当一台主机的链路过载（无论是传入流量还是传出流量）时，TCP 会尝试在所有活动连接之间平均分配可用带宽。这种方法也被称为 “公平调度”。
不幸的是，像这样的调度策略在负载情况下表现不佳是众所周知的。当接收到若干条大消息时，带宽共享会导致所有这些消息都缓慢完成处理。诸如 SRPT（最短剩余处理时间）这类运行至完成的方法能提供更优的整体响应时间，因为它们每次会将所有可用资源都投入到单个任务上，从而确保该任务能快速完成。使用 TCP 来实现运行至完成是很困难的，因为 TCP 并不知晓消息边界的相关信息；因此，它无法得知一个任务何时算是“完成”了。
此外，尽管名为“公平调度”，但 TCP 的这种做法对短消息极为不利。图 1 展示了在负载很重的网络上运行时，不同大小的消息的往返延迟相较于在无负载网络上相同大小的消息是如何变慢的。使用 TCP 时，短消息的延迟变慢情况几乎比最长消息要严重 10 倍。DCTCP（数据中心传输控制协议）在一定程度上缩小了这一差距，但短消息受到的影响仍比长消息严重 3 倍。在数据中心环境中，短消息的延迟至关重要，所以这种区别对待是存在问题的。

![](Pasted%20image%2020241120133135.png)
图 1：TCP、DCTCP 和 Homa 的 Linux 内核实现的第 99 百分位减速与消息长度的函数关系，在一个具有 25 Gbps 网络链路且平均利用率为 80%的 40 节点 CloudLab 集群上运行（详见）。工作负载基于在 Facebook 的 Hadoop 集群上测量的消息大小分布。减速是指在负载集群上消息的往返时间除以在无负载系统中相同长度的 Homa 消息的时间。

## 发送端驱动的拥塞控制

TCP 的拥塞控制由发送方驱动，当发送方检测到拥塞时，会主动降低其数据包的传输速率。发送方并不直接了解拥塞情况，拥塞可能发生在核心网络架构中，也可能发生在架顶式交换机与接收方之间的边缘链路处，所以它们依赖与缓冲区占用相关的拥塞信号。在最坏的情况下，交换机会出现队列溢出，数据包会被丢弃，从而导致超时。更常见的情况是，当队列长度达到一定阈值时，交换机会生成显式拥塞通知（ECN），或者发送方会检测到由于排队而导致的往返时间增加；一些较新的方法会使用可编程交换机来生成更精确的信息，比如确切的队列长度。然后发送方会利用这些信息来减少数据包的传输。
TCP（传输控制协议）中的拥塞控制受到两个限制的束缚。首先，只有在缓冲区被占用时才能检测到拥塞；这实际上就保证了在网络负载时会出现一些数据包排队的情况。其次，TCP 没有利用现代网络交换机中的优先级队列。因此，所有数据包都被同等对待，长消息（此时吞吐量比延迟更重要）产生的队列会给短消息造成延迟。
这些限制导致了一种“两难困境”，即很难同时优化延迟和吞吐量。要确保短消息的低延迟，唯一的办法就是让网络中的队列长度接近于零。然而，这存在缓冲区下溢的风险，即尽管有流量可以使用链路，但链路却处于空闲状态；这会降低长消息的吞吐量。在面对流量波动时要保持链路得到充分利用，唯一的办法就是允许在稳定状态下积累缓冲区，但这会给短消息造成延迟。
此外，发送方大约需要一个往返时间（RTT）才能察觉到流量的变化，所以发送方必须依据过时的信息来做决策。随着消息越来越短且网络速度越来越快，会有越来越多的消息在不到一个往返时间内就完成传输，这使得发送方所接收到的信息越来越不可靠。
拥塞控制在 TCP 以及其他诸如远程直接内存访问（RDMA）等流传输方式中都已得到了广泛的研究。这些努力已经带来了相当大的改进，但如果不打破 TCP 的一些基本假设，延迟与吞吐量之间的两难困境不太可能得到完全解决。

## 按序数据包交付

TCP（传输控制协议）假定数据包会按照发送方发送的相同顺序到达接收方，并且认为乱序到达意味着数据包丢失。这严重限制了负载均衡，导致硬件和软件方面都出现热点问题，进而产生较高的尾延迟。
在数据中心网络中，执行负载均衡最有效的方法是进行数据包喷洒（packet spraying），即每个数据包都通过交换架构独立路由，以平衡链路上的负载。然而，数据包喷洒无法在 TCP（传输控制协议）中使用，因为这可能会改变数据包到达目的地的顺序。相反，TCP 网络必须使用流一致路由，即来自给定连接的所有数据包都通过网络架构遵循相同的路径。流一致路由可确保数据包按序交付，但实际上这导致了即使在整体网络负载较低时，网络核心中也会存在过载链路。要造成拥塞，只需要两个大流量哈希到同一条中间链路即可；这个热点问题会在这些流量的存续期间一直存在，并给同样经过受影响链路的其他任何消息造成延迟。
我推测，流一致路由几乎是导致数据中心网络核心出现所有拥塞情况的原因。
按序数据包交付也会在软件中引发热点问题。例如，Linux 通过在多个核心间分配传入数据包的处理任务来实现软件层面的负载均衡；这对于维持高数据包速率至关重要。每个传入数据包在到达应用程序（可能位于第三个核心上）之前，会在内核的两个不同核心上进行处理。为了确保按序数据包交付，给定 TCP 连接的所有数据包都必须经过相同的核心处理序列。当两个或更多活动连接哈希到同一个核心时，这就会导致核心负载不均衡；同样，只要这些连接处于活动状态，热点问题就会持续存在。中的测量结果表明，热点问题是 TCP 中由软件导致的尾延迟的主要成因。

> 更正（2023 年 1 月）：本节的第一段内容有误。在 TCP 中，乱序到达的数据包不一定会触发数据包重传。诸如三次重复确认（triple-duplicate ACKs）和快速确认（RACK）等机制允许 TCP 在一定程度上容忍数据包的重新排序而无需进行重传。然而，专家告诉我，数据中心网络中的不对称性可能会导致严重的数据包重新排序，超出 TCP 的容忍范围。此外，网络接口卡（NICs）和 Linux 网络栈中的性能优化措施，如大接收卸载（LRO）和通用接收卸载（GRO），即使在数据包重新排序程度较轻的情况下也会失效，从而导致显著的性能下降。因此，网络硬件和 Linux 内核软件都会尝试维持数据包的排序，进而导致了上述问题。

# TCP 无药可救

针对 TCP 存在的问题，一种可能的应对办法是采取渐进式方法，即在保持应用兼容性的同时逐步解决这些问题。已经有过许多此类尝试，并且也取得了一些进展。然而，这种方法不太可能成功：问题实在太多了，而且它们在 TCP 的设计中根深蒂固。
举个例子，来考虑一下拥塞控制。近年来，TCP 的这一方面可能比其他任何方面都受到了更多的研究，并且已经设计出了许多新颖巧妙的技术。最早的技术之一是 DCTCP（数据中心传输控制协议）；它在尾延迟方面有显著的改善（见图 1），并且已经得到了广泛的应用。更近一些的方案，如 HPCC（高性能拥塞控制），提供了令人瞩目的额外改进（它们未包含在图 1 中，因为它们没有 Linux 内核实现）。然而，所有这些方案都受到 TCP 基本方面的限制，比如其基于缓冲区占用的微弱拥塞信号、无法使用交换机优先级队列以及按序交付的要求。只有打破 TCP 的一些基本假设，才有可能实现显著的额外改进。图 1 中的 Homa 曲线表明，相当大的改进是有可能的（尽管图 1 中未显示，但 Homa 也比 HPCC 等更新的方案能提供更好的尾延迟）。
渐进式方法存在的问题之一在于，TCP 存在诸多问题且这些问题相互关联。例如，由于缺乏消息边界，使得实施最短剩余处理时间（SRPT）机制变得困难，并且限制了可用于拥塞控制的信息量。因此，在能够看到明显改进之前，TCP 的许多不同部分都必须进行更改。
此外，TCP 的问题不仅涉及它的实现方式，还涉及它的应用程序编程接口（API）。为了在数据中心实现性能最大化，TCP 将不得不从基于流和连接的模型转变为基于消息的模型。这是一个会影响到应用程序的根本性改变。一旦应用程序受到影响，我们不妨同时解决 TCP 的所有其他问题。
关键在于，TCP 已经没有什么值得保留的部分了。我们需要一种在各个重要方面都与 TCP 不同的替代协议。幸运的是，这样一种协议已经存在了：Homa（高性能消息协议）。Homa 的存在证明了 TCP 的所有问题实际上都是可以解决的。

# Homa

Homa 代表了对数据中心网络传输的一次全新设计。它的设计参考了 TCP 存在的问题，以及使用无限带宽（Infiniband）和远程直接内存访问（RDMA）来实现大规模数据中心应用的经验。Homa 的设计在第 3 节所讨论的各个方面都与 TCP 不同。本节简要总结了 Homa 的特点；如需详细内容，请参阅 xxxx。

## 消息

Homa 是基于消息的。更确切地说，它实现了远程过程调用（RPC），即客户端向服务器发送请求消息，并最终接收响应消息。消息的主要优势在于，它们向传输层提供了可调度的单元。这使得负载均衡更加高效：多个线程可以安全地从单个套接字读取数据，并且基于网络接口卡（NIC）的协议实现可以通过内核旁路将消息直接分发给一组工作线程。具有明确的消息边界还能够在传输中实现诸如最短剩余处理时间（SRPT）这样的运行至完成调度，并提供更强大的拥塞信号（见下文）。
与流相比，消息有一个劣势：对单个大型消息的实现进行流水线操作比较困难。例如，在整个消息全部接收完毕之前，应用程序无法接收该消息的任何部分。因此，单个大型消息的延迟会比通过流发送相同数据的延迟更高。不过，可以通过并行发送多个消息来处理大型数据传输，这就允许在消息之间进行流水线操作。

## 无连接

Homa 是无连接的。不存在连接建立的开销，并且应用程序可以使用单个套接字来管理与任意数量对等方的任意数量的并发远程过程调用（RPC）。每个 RPC 都是独立处理的：在并发 RPC 之间不存在顺序保证。
Homa 所维护的状态可分为三大类：

- 套接字：Homa（高性能消息协议）中每个套接字的状态大致与 TCP（传输控制协议）中的相当，但 Homa 应用程序使用单个套接字就能应付，而 TCP 应用程序则需要为每个对等方配备一个套接字。
- 远程过程调用（RPC）：对于每个正在进行的远程过程调用，Homa 会保留大约 300 字节的状态信息。一旦该远程过程调用完成，这些状态信息就会被丢弃，所以状态信息的总量与正在进行的远程过程调用的数量成正比，而不是与对等方的总数成正比。
- 对等方：每台运行 Homa 协议的主机为其他每台主机保留大约 200 字节的状态信息，其中大部分是 IP 层的路由信息。这比 TCP（传输控制协议）为每个连接所维护的 2000 字节状态信息要少得多。
  尽管 Homa（高性能消息协议）不存在连接的概念，但它能确保远程过程调用（RPC）的端到端可靠性（或者在发生不可恢复的网络或主机故障后报告错误）。应用程序无需设置额外的超时机制。诸如流量控制、重试和拥塞控制等机制是利用每个远程过程调用的状态来实现的；一种理解 Homa 的方式是，它为每个远程过程调用实现了一种短暂且轻量级的连接。

## SPRT

Homa（高性能消息协议）实施了最短剩余处理时间（SRPT）调度策略以优先处理较短的消息。为此它采用了多种技术，其中最值得注意的是它利用了现代交换机所提供的优先级队列。这使得优先级较高（较短）的消息能够绕过为优先级较低（较长）的消息所排队的数据包。如图 1 所示，与 TCP（传输控制协议）或 DCTCP（数据中心传输控制协议）相比，这在尾延迟方面带来了相当大的改进。各种长度的消息都能从 SRPT 中受益：即使是最长的消息，在 Homa 协议下的延迟也比在 TCP 或 DCTCP 协议下显著更低。
关于最短剩余处理时间（SRPT）策略，一个潜在的担忧是最长的消息可能会遭受过高的尾延迟，甚至出现饥饿（得不到处理）的情况。但在实际应用中尚未观察到这个问题，而且即便采用对抗性的测试方法也很难引发该问题。尽管如此，Homa 协议的 Linux 实现版本包含了一项额外的保障措施：每台主机的一小部分带宽（通常为 5%至 10%）会专门分配给最旧的消息，而非最短的消息。这样可以在极端情况下避免饥饿现象，并改善长消息的尾延迟，同时仍然采用运行至完成的调度方式。
Homa 对优先级队列的使用消除了第 3.4 节中所讨论的在延迟和带宽之间“两难抉择”的权衡问题。Homa 有意允许一些来自较长消息的缓冲区数据在低优先级队列中累积（过度分配）；这些做法确保了链路的高利用率。由于短消息使用较高优先级队列，它们仍然能够实现低延迟。

## 接收方驱动的拥塞控制

Homa 是由接收方而非发送方来管理拥塞情况的。这是合理的，因为拥塞主要发生在接收方的下行链路（正如下文 5.5 节所讨论的，Homa 消除了核心拥塞问题）。接收方了解所有传入的消息，因此它在管理这种拥塞方面处于更有利的位置。当发送方发送一条消息时，它可以单方面发送一些非调度数据包（足以覆盖往返时间），但其余的调度数据包可能只能根据接收方的授权才能发送。通过这种机制，接收方可以限制其下行链路的拥塞情况，并且它还利用这些授权来优先处理较短的消息。
消息提供了一种强大的拥塞信号，这在基于流的协议中是不存在的。尽管消息的到达是不可预测的，但一旦看到消息的第一个数据包，就知道了该消息的总长度。这使得能够采取主动的拥塞控制方法，比如在这条消息的传输期间限制其他消息的传输，并在该消息传输完成后再次增加它们的传输量。相比之下，TCP 只能基于缓冲区占用情况做出被动反应。
如果许多发送方同时发送非调度数据包，就可能会出现集中流入（Incast）现象，但 Homa（高性能消息协议）的远程过程调用（RPC）导向使得可以采取一种简单的缓解措施；详情请参阅关于 Homa 的相关论文。

## 数据包乱序

Homa（高性能消息协议）的一个关键设计特点是它能够容忍乱序数据包的到达。这为负载均衡提供了相当大的灵活性。例如，可以使用数据包级别的分散传输方式将数据包在网络结构中进行分发，而不像 TCP（传输控制协议）那样采用流一致性路由。我推测，如果 Homa 得到广泛部署，只要核心网络没有系统性过载，核心拥塞将不再作为一个重大的网络问题而存在。Homa 对乱序到达数据包的容忍度也为软件中的负载均衡提供了更多灵活性。

## 相关工作

最近有几篇论文声称发现了 Homa（高性能消息协议）存在的问题并且/或者声称提高了其性能，这些论文包括《Aeolus》、《PowerTCP》以及《DcPIM》。然而，所有这些论文都存在重大缺陷，比如没有正确实现 Homa 协议，或者是在人为限制条件下对其进行测量（例如，《Aeolus》在交换机中使用静态缓冲区分配）。关于这些论文的更详细讨论，请参阅 Homa 维基百科。Homa 维基百科还包含了关于 Homa 的各种其他信息，并且将来会进行更新以纳入新的信息和相关研究成果。

# 关于 Infiniband

除了 Homa（高性能消息协议）之外，还有其他可替代 TCP（传输控制协议）的方案，但似乎没有哪种能满足数据中心计算的需求。其中最知名的替代方案之一是无限带宽（Infiniband），它已在高性能计算（HPC）领域得到广泛应用，并且最近通过以太网之上叠加远程直接内存访问（RDMA）应用编程接口的远程直接内存访问以太网（RoCE）技术，在数据中心的使用也日益增多。
RDMA 的主要优势在于，它能在空载网络上提供极低的延迟。它通过将传输协议的实现转移到网络接口卡（NIC）上，并允许用户进程绕过内核直接与网络接口卡进行通信来实现这一点。Infiniband/RDMA 的网络接口卡因其极高的性能而实至名归。
然而，远程直接内存访问（RDMA）存在着与传输控制协议（TCP）的大部分相同的问题。它基于流和连接（RDMA 也提供不可靠的数据报，但这些数据报存在着与用户数据报协议（UDP）类似的问题）。它要求数据包按顺序交付。其基于优先级流控制（PFC）的拥塞控制机制虽与 TCP 的不同，但同样存在问题。而且，它没有实现最短剩余处理时间（SRPT）优先级机制。
远程直接内存访问（RDMA）还有一个额外的劣势，那就是基于网络接口卡（NIC）的协议实现是专有的，所以很难确切了解它们的运行方式并排查问题。举个例子，RAMCloud 项目发现无限带宽（Infiniband）存在一些性能异常情况，尤其是在高负载时；在大多数情况下，由于其实现方式的封闭性，无法对这些问题进行追踪排查。
未来的传输协议实现应该采用无限带宽（Infiniband）的 kernel bypass 方法，但似乎无限带宽技术本身不太可能解决传输控制协议（TCP）的所有问题。

# 展望未来

很难想象还有比传输控制协议（TCP）根基更稳固的计算标准了，所以要取代它将会十分困难。更糟糕的是，Homa（高性能消息协议）（或者任何能解决 TCP 所有问题的协议）都需要对应用程序编程接口（API）进行变更，这意味着应用程序代码也必须加以修改。鉴于有数量极为庞大的应用程序是直接基于套接字接口编写代码的，要把它们全部修改好，这一任务至少在近期看来似乎是难以完成的。
幸运的是，没必要针对所有应用程序都替换传输控制协议（TCP）。对新传输协议需求最为迫切的应用程序是那些较新的大规模数据中心应用程序。这些应用程序中的大多数并不会针对套接字应用程序编程接口（API）进行编码；相反，它们是构建在为数不多的几个远程过程调用（RPC）框架（如 gRPC 或 Apache Thrift ）之一之上的。让一种新协议得到广泛应用的最简单方法就是将其与主要的 RPC 框架整合起来。这是一项相当易于管理的任务，而且一旦完成这项任务，使用这些框架的应用程序只需进行极少的工作甚至无需做任何工作就可以切换到 Homa（高性能消息协议）。
框架整合方面的工作已经展开：目前针对 C++应用程序已经有了一个适用于 Homa（高性能消息协议）的 gRPC 驱动程序，并且针对 Java 的支持也正在进行中。这项工作是基于 Homa 的 Linux 内核实现版本开展的。

# 结论

对于数据中心计算而言，TCP（传输控制协议）是不合适的协议。TCP 设计的方方面面都存在问题：没有任何值得保留的部分。如果我们想要消除“数据中心税”（指数据中心因使用传统协议等因素导致的效率损耗等类似代价），就必须找到一种方法，将大多数数据中心的流量转移到一种截然不同的协议上。Homa（高性能消息协议）提供了一种替代方案，似乎能够解决 TCP 的所有问题。要让 Homa 得到广泛应用，最好的办法是将其与大多数大规模数据中心应用所依托的远程过程调用（RPC）框架进行整合。
