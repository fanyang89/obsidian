# 背景

https://cs.smartx.com/cases/detail?id=1030826

从日志中看到，Node1 05:43:27~31 之间异常（异常前 Node 1 是 meta leader）之后，没有触发 meta 的重新选举。期间 ZK 服务正常，各个服务向 ZK 请求 meta leader 依旧返回之前的 meta leader 。但是此时 Node 1 上的 Meta 无法响应请求。存活的 2 个 chunk 都进入了 Session expired 状态。不再可以提供服务。
在 Node1 重启之后，ZK 视角下的原 Meta 连接在较长时后才消失。因此推测是 ZK 未能正确处理旧的 Meta leader 离线，没有正常的触发选举，而不是 Node1 的 meta leader 在卡死期间依旧保持存活而维系了自己在 zk 记录中的存在状态。需要进一步调查。（不成立）

# 日志

```bash
# node1 meta
I0625 05:43:23.800374  5366 nfs_server.cc:1115] [SETATTR]: [REQUEST]: id: "97c3830c-9c94-4569" sattr3 { atime_how: SET_TO_SERVER_TIME atime { seconds: 0 nseconds: 0 } mtime_how: SET_TO_SERVER_TIME mtime { seconds: 0 nseconds: 0 } }, [RESPONSE]: ST:, name: "59f43b6e-c54b-11eb-b896-f8f21ee58960" pool_id: "4a465dd0-d0f9-4464-b95d-b23e7a8acc60" id: "97c3830c-9c94-4569" parent_id: "20257277-830
```

```bash
# node1 zk
2022-06-25 04:53:20,687 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x10000030a76c74f, likely client has closed socket
2022-06-25 04:59:41,571 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x10000030a76c768, likely client has closed socket
2022-06-25 05:16:21,650 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x10000030a76c7a0, likely client has closed socket
2022-06-25 05:20:54,784 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x10000030a76c7b1, likely client has closed socket
2022-06-25 05:27:30,752 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x10000030a76c7c5, likely client has closed socket
2022-06-25 05:28:24,814 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x10000030a76c7c8, likely client has closed socket
2022-06-25 05:33:24,818 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x10000030a76c7d8, likely client has closed socket
2022-06-25 08:08:34,322 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:NIOServerCnxn@383] - Exception causing close of session 0x0: ZooKeeperServer not running
2022-06-25 08:08:36,813 [myid:1] - WARN  [QuorumPeer[myid=1]/10.1.88.101:2181:Learner@387] - Got zxid 0xa00e58f5a expected 0x1
2022-06-25 08:08:36,621 [myid:1] - WARN  [QuorumPeer[myid=1]/10.1.88.101:2181:Follower@119] - Got zxid 0xa00e58f5d expected 0x1
2022-06-25 08:08:46,210 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:ZooKeeperServer@908] - Connection request from old client /10.1.88.101:51058; will be dropped if server is in r-o mode
2022-06-25 08:08:57,880 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x10000028c230008, likely client has closed socket
2022-06-25 08:09:20,012 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x10000028c23000b, likely client has closed socket
2022-06-25 08:09:31,601 [myid:1] - WARN  [NIOServerCxn.Factory:/10.1.88.101:2181:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x10000028c23000d, likely client has closed socket
```

```bash
# node2 chunk
W0625 05:43:50.012274  3986 zbs_client.cc:641] [Retry IO ON ERROR]: [IOCTX]: VEXTENT_READ seq: 24875104435, volume: 5bddc847-a741-4c32-b09f-74afe94e5cca, in_progress: 1, return_not_alloc: 0, resize: 0, etry_times: 0, last_retry_ms: 0 , vextent no: 1603, data_len: 8192, extent_offset: 266682368, pid: 0, owner_ip: , owner_port: 0  st:
Traceback:
[ETimedOut]: ProtoAsyncClient meet timeout: expect timeout ms: 10000 elapsed ms: 10098 socket: 10.1.88.102:56828-->10.1.88.101:10100
# node3 同，连不上 node1 meta leader
```

也就是此时 zk 仍然可以返回当前的 meta 是 node1
zk 此时是正常工作的。

![](assets/Pasted%20image%2020220628175215.png)
zbs-scavenger 每 5min 运行一次。但是时间出现了断层。

```bash
# node1 meta
I0625 05:43:23.800374  5366 nfs_server.cc:1115] [SETATTR]: [REQUEST]: id: "97c3830c-9c94-4569" sattr3 { atime_how: SET_TO_SERVER_TIME atime { seconds: 0 nseconds: 0 } mtime_how: SET_TO_SERVER_TIME mtime { seconds: 0 nseconds: 0 } }, [RESPONSE]: ST:, name: "59f43b6e-c54b-11eb-b896-f8f21ee58960" pool_id: "4a465dd0-d0f9-4464-b95d-b23e7a8acc60" id: "97c3830c-9c94-4569" parent_id: "20257277-830"

# node2 zk 间隔 8s（根据之前的调查，相差一个 tickTime 是正常现象）
2022-06-25 05:43:31,656 [myid:2] - WARN  [QuorumPeer[myid=2]/10.1.88.102:2181:LearnerHandler@702] - Closing connection to peer due to transaction timeout.
2022-06-25 05:43:31,657 [myid:2] - WARN  [LearnerHandler-/10.1.88.101:55810:LearnerHandler@661] - ******* GOODBYE /10.1.88.101:55810 ********
2022-06-25 05:43:31,657 [myid:2] - WARN  [LearnerHandler-/10.1.88.101:55810:LearnerHandler@673] - Ignoring unexpected exception
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1220)
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335)
	at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:339)
	at org.apache.zookeeper.server.quorum.LearnerHandler.shutdown(LearnerHandler.java:671)
	at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:664)
2022-06-25 05:43:31,658 [myid:2] - WARN  [NIOServerCxn.Factory:/10.1.88.102:2181:ZooKeeperServer@908] - Connection request from old client /10.1.88.102:38684; will be dropped if server is in r-o mode

# node1 chunk
$ stat -x ./192_168_2_101_c20dbab2-f45d-11ec-b511-1a21aa1967d6/scvm_logs/zbs-chunk/zbs-chunkd.log.20220615-175901.3570
  File: "./192_168_2_101_c20dbab2-f45d-11ec-b511-1a21aa1967d6/scvm_logs/zbs-chunk/zbs-chunkd.log.20220615-175901.3570"
  Size: 28177459     FileType: Regular File
  Mode: (0644/-rw-r--r--)         Uid: (  501/ fanyang)  Gid: (   20/   staff)
Device: 1,17   Inode: 8619008    Links: 1
Access: Tue Jun 28 15:56:28 2022
Modify: Sat Jun 25 05:43:24 2022
Change: Tue Jun 28 15:56:30 2022
 Birth: Sat Jun 25 05:43:24 2022
```

所以，node1 失能的最后时间点是：`0625 05:43:23` 到 `05:43:24` 左右。

# 疑问

1. 为什么所有的服务都卡住了？
   根据 case 描述，应该是单盘故障导致 HBA 故障，导致系统无法完成 I/O。所有服务均无法写入。

2. 为什么 zk 仍然正常工作？
   故障时间点后，实际上 node1 zk 已经无法正常工作。剩余两节点 zk 正常工作，zk 集群正常工作。

# 怀疑 watchdog

http://gerrit.smartx.com/c/zbs/+/25219
怀疑 meta watchdog 没有正常工作。尝试寻找原因。
看起来是卡 logger。寻找测试方式。等 wp 结论。

# 是否真的存在数据不一致

另一个问题是，是否存在 ZooKeeper 数据不一致现象。
Node1: ZNode count=32672, session count=9
Node2: ZNode count=32681, session count=9
Node3: ZNode count=32689, session count=9
从目前的情况来看，存在数据不一致。

那么，是哪些 znode 不一致呢？
Session 三节点均一致。

```bash
$ python3 ./compare.py
compare node1 and node2
lhs:  32671 , rhs:  32680
diff1: 9 {'/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eacd', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac2', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac3', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eacc', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac5', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac8', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac7', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac4', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac6'}
diff2: 0 set()

compare node1 and node3
lhs:  32671 , rhs:  32688
diff1: 17 {'/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eacd', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac2', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead4', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac3', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead2', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead3', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eacc', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac5', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead5', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac8', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac7', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac4', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eacf', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eac6', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead1', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead0', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eace'}
diff2: 0 set()

compare node2 and node3
lhs:  32680 , rhs:  32688
diff1: 8 {'/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead4', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead2', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead3', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead5', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eacf', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead1', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9ead0', '/zbs/meta/__mj_j/0000000000d9e955/0000000000d9eace'}
diff2: 0 set()
```

可以看到，node3 拥有着最多的 znode 数量。并且 node1 和 node2 znode 的差值加起来正好是 node1 和 node3 的差
即：node1 -- 7 znodes -- node2 -- 8 znodes -- node3

```
/zbs/meta/__mj_cluster/10.1.88.101:10100
  cZxid = 0x00000a00e58f15
  ctime = Sat Jun 25 08:08:19 CST 2022
  mZxid = 0x00000a00e58f15
  mtime = Sat Jun 25 08:08:19 CST 2022
  pZxid = 0x00000a00e58f15
  cversion = 0
  dataVersion = 0
  aclVersion = 0
  ephemeralOwner = 0x20000031a22e220
  dataLength = 8
```

检查是哪些 znode 在故障期间被创建。故障时间为：`0625 05:43:23~08:08:19`
这段时间内创建的都是选举用的 ephemeral 节点。

ZOOKEEPER-3642: Fix potential data inconsistency due to DIFF sync after partial SNAP sync
