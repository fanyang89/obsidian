主题是解决空间不够的时候，局部化分配不能生效的时候怎么办。
再加上一些避免策略边界震荡就可以了。
一条专利就这两个点应该就够了。

# 弹性负载策略

按照集群中数据空间已分配比例最高节点的磁盘空间已分配比例 p 作为标准，策略目标优先级调整如下：

1. 低负载状态（p < P_medium），此时节点间的容量均衡被忽略，以本地优先与数据局部化为目标分配与调整数据，优先级次序为 ： ②本地优先 =  ① 拓扑安全 >  ③局部化分配；
2. 中负载状态( P_medium  <= p < P_high），此时新副本分配还是按照本地优先与数据局部化为目标，但是迁移扫描将根据当前 IO 状态，尝试迁移部分非本地且非活跃数据（非活跃目前指的是 Extent 没有活跃的 Owner，通常这代表最近 1 小时没有 IO 发生，或者 extent 存在 owner，但是 owner 不是迁移的源。非本地指的迁移的源 Chunk 不是  Extent 的 Prefer Local。新分配的数据一定持有 Owner，所以不会被判定为非活跃数据，与副本分配的策略不会有大范围的冲突）以达到容量均衡，分配时优先级次序与低负载时相同。再平衡时对非本地活跃数据的优先级次序为： ②本地优先 =  ①拓扑安全 >  ④容量均衡 > ③局部化分配 ；
3. 高负载状态（P_high <= p），分配时的数据局部化目标将暂时被忽略，以本地优先 + 容量均衡为目标进行数据分配，数据调整时除中负载时的非本地且非活跃数据化之外还会尝试迁移包含本地的非活跃数据（即数据只要 owner 不存在，或者 owner 不是 extent 迁移源。即便 extent 的 prefer local 是迁移源也会被迁移）。此时根据 Extent 的 prefer local 节点本身的负载在初次分配副本时有所不同：
   1. Prefer local 节点自身的容量没有超过 P_high%，优先级次序为 ②本地优先 =  ①拓扑安全 > ④容量均衡
   2. Prefer Local 节点本身容量已经超过 P_high%， 侧放弃本地优先，仅采用 ①拓扑安全 > ④容量均衡策略

节点的已分配比例取值方式为 Provisioned Data Space / Valid Data Space 。其中 Valid Data Space 代表健康的持久化存储层的空间（Partition），Provisioned Data Space 代表 Meta 已经分配给节点上持有的数据空间。需要注意的是，Provisioned Space 和节点的实际可用空间可能存在差异。节点的实际使用空间有可能尚未回收，特别是 LSM 1 制作大量快照后删除时。但节点的实际使用空间最终会趋向于 Meta 分配给节点的数据空间。详细的空间定义参见：[数据存储空间概念](https://docs.google.com/document/d/1oOZ6CENaLFBU_AG6tZ4nnxv1CFUNvv3ND_NWVGVN2PY/edit#)。

在 4.0.10 及之后 medium% 的默认值为 75%，high% 的默认值为 85%。在 4.0.10 之前 medium% 的默认值为 60%,  high% 的默认值为 80%。

只有持有有效数据空间的节点的分配比例才会作为集群负载的判定因素，这代表如下节点的负载情况不会参与判定：

- 节点或异常失联状态，所有数据空间状态处于未知；
- 节点上没有挂载任何 Partition， 上报的有效数据空间大小为 0；
